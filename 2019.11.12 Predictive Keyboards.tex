\documentclass[12pt,a4paper]{article}
\usepackage[top=30pt,bottom=30pt,left=48pt,right=46pt]{geometry}
\usepackage{amsmath,amssymb,amsthm,bm,hyperref}
\setlength\itemsep{2mm}

\author{Thomas Hofmann}
\title{Predicive Keyboard Models}
\begin{document}
\maketitle

\subsection*{Phase 1}

The conceptually simplest task, which at the same time is most transparent to the user, is to suggest auto-complete options. It seems advisable to start with something simple and to add smartness on top in a controllable manner. A good starting point is a simple prefix tree (aka trie), possibly compressed into a DFA, created from a fixed vocabulary of words. Each edge should be labeled with a probability (quantized on log-scale) of continuation by taking word frequencies into account (i.e.~the vocabulary comes with term frequencies). Upon each keystroke event, a probability for possible successions is computed. If we abstain from spell corrections, then we would typically consider $1-3$ possibilities for the characters in the vincinity of the stroke location, using a simple probabilistic model. This likelihood will be combined with the prior to maintain a (search) beam of  prefixes with the highest posterior probability.
\begin{align}
\text{prior model:} \quad & P(a^n | a^{n-1} \cdots a^1) \\
\text{observation model:} \quad & P((x,y) | a_i)\\
\text{posterior:} \quad & P(a^1 \cdots a^n | (x^1,y^1) \cdots (x^n,y^n)) \propto \left[ \prod_t p(x^t,y^t|a^t) \right] \left[ \prod_{t=1}^n p(a^t|a^{t-1} \cdots a^1) \right]
\end{align}
For each prefix in the beam there is then a top list of completions with their probability. 




\newpage



\bibliography{th}
\bibliographystyle{acm}

\end{document}
