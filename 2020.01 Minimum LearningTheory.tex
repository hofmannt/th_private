\documentclass[12pt,a4paper]{article}
\usepackage[top=50pt,bottom=50pt,left=48pt,right=46pt]{geometry}
\usepackage{amsmath,amssymb,amsthm,bm,bbm,hyperref}
\usepackage{titlesec}
\usepackage{graphicx}
\graphicspath{{./images/}}

\input{defs}

\author{Thomas Hofmann}
\title{Machine Learning: The Conceptual Minimum }
\date{\today}

\begin{document}
\maketitle

\section{Basic Concepts}

\paragraph*{Models and  Accuracy}

Let us consider a computational device, a black box that takes \textit{inputs} $x$ and produces \textit{outputs} $y$ via application of a function $f$.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.56\textwidth]{Blackbox}
    \caption{Black box device}
    \label{fig:black-box-device}
\end{figure}

\noindent We would like to think of this device as a \textit{model} for some aspect of the world. For instance, \textit{in reality} outputs may depend on inputs via a mechanism, the laws of nature or simply by conventions. The model will typically be imperfect and outputs will only be correct with a certain accuracy, allowing the possibility of errors.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Approx}
    \caption{Approximate model}
    \label{fig:approximate-model}
\end{figure}

\noindent Formally, denote by $e_f$ the Boolean error function of $f$,
\begin{align}
    e_f: (x,u) \mapsto \mathbb{I}[f(x) \neq u] \in \{0,1\}\,,
\end{align}
which is central in assessing the  \textit{accuracy} of a model, i.e.~the probability of it being correct. Note that some conditions $x$ may never occur in reality, others may occur with smaller or larger probability. Moreover, the relationship between conditions and responses may not be deterministic: the same $x$ may not always result in the same $u$.  It is thus best to work in a probabilistic setting, assuming data is generated by a  \textit{data generating distribution} $\mc D$. We can then define the accuracy as one minus the expected error rate, 
\begin{align}
    \text{Accuracy of $f$} := 1- \mathbf{E}[e_f]
\end{align}
Note that as the data generating distribution is usually not explicitly given, we cannot directly compute the accuracy. Instead we will work with a sample $\mathcal{S}= \{(x_i,u_i) \stackrel{\text{\tiny  iid}}\sim \mc D: i=1,\dots,n\}$ and the induced  sample error rate 
\begin{align}
    \mc {S}[e_f] := \frac{1}{n} \sum_{i=1}^n e_f(x_i,u_i)\,.
\end{align}
Eventually for large enough samples, the law of large numbers will guarantee convergence of frequencies to probabilities. In fact, the Central Limit Theorem tells us that in distribution 
\begin{align}
\sqrt n (\mb E[e] - \mc S[e]) \stackrel{}\to \mc N(0,\sigma^2), \quad \sigma^2 = \mb{V}[e^2] \le \tfrac 14\,,
\end{align}
from which it follows that
\begin{align}
\lim_{n \to \infty} \Pr\left[ \sqrt n (\mb E[e] - \mc S[e] ) \le \epsilon \right] = \Phi(4\epsilon), \quad \text{where} \; \Phi = \text{standard normal CDF}.
\end{align}
Roughly speaking for large enough $n$ we get -- NEEDS MORE DETAIL --
\begin{align}
\Pr\left[ \mb E[e] - \mc S[e] \le \epsilon \right] \lessapprox \Phi\left(\frac{4 \epsilon}{\sqrt n} \right) \in \mb O\left( \exp[-16\epsilon^2/n]\right)
\end{align}



Let us focus on the special case of perfect accuracy on the sample, $\E_{\mc S}[e_f]=0$, a situation not uncommon in practical situations where powerful models can often reduce the empirical error to zero. 




\newpage






However, what guarantees can be given, not asymptotically, but based on a finite sample of size $n$?


\end{document}


\subsection{Validating Fixed Hypothesis}

\paragraph{Setting}
The canoncial problem studied in machine learning is binary classification. The goal is to identify a mapping $f: \mc X \subseteq \Re^d \to \{0,1\}$  assigning binary labels to inputs, i.e.~vectors of measurements or features. Assume that we have a hypothesis $f$ and a (multi-)set of examples $\mc S = \{ (\x_1,y_1), \dots, (\x_n,y_n)\} $ drawn independently from an underlying, fixed, yet unknown, distribution $P$. Imagine that we use $\mc S$ to assess the empirical error rate of $f$. Define the empirical and true error rate, respectively,  
\begin{align}
e_f(S) := \frac 1n \sum_{i=1}^n  \mathbb I[ f(\x_i) \neq y_i], \quad e_f := \E \left[ \mathbb I[ f(\x) \neq y] \right] \,.
\end{align}

\paragraph{Special Case: Zero Sample Error}
Clearly the sample error may be non-representative of the true error and we can only aim for probabilistic bounds over random draws of the sample. If  $e_f = \epsilon$ (a fixed, non-random quantity), then the probability of $f$ making $r$ errors on an $n$-sample is given by the binomial law,,
\begin{align}
\Pr\{e_f(\mc S) = r/n\} = \binom{n}{r} \epsilon^{r} (1-\epsilon)^{n-r}, \quad 1 \le r \le n
\end{align}
Particularly, if $f$ is correct on $\mc S$, i.e.~$r=0$, then  
\begin{align}
\label{eq:binom-exp-bound}
\Pr\{e_f(\mc S) = 0\} =  (1-\epsilon)^{n} = \exp[n\ln(1-\epsilon)] \le \exp[-n \epsilon]
\end{align}
where the latter inequality follows from the concavity of the logarithm. 
%
\begin{detail}
For any $t_0, \, t>0$ by concavity 
\begin{align*}
\ln(t) \le \ln(t_0) + (t-t_0) \frac{d \ln}{dt}_{\Big| t=t_0} = \ln (t_0) + \frac{t-t_0}{t_0}\,.
\end{align*}
Now by setting $t_0=1$ and $t=1-\epsilon$ one gets the right-hand side bound in Eq.~\eqref{eq:binom-exp-bound}, which is useful in cases where $\epsilon$ is small as it gets tight at $\epsilon=0$.
\end{detail}
%
\noindent  Procedurally, we typically want to specify a confidence level $\delta<1$ and derive a bound on $\epsilon$. This can be done by simple manipulations, e.g.~starting from \eqref{eq:binom-exp-bound}
\begin{align}
\exp[-n\epsilon] \stackrel !\ge\delta \iff -n \epsilon \ge \ln(\delta) \iff \epsilon \le \frac{\ln(1/\delta)}{n} 
\end{align}
One follows this patterns in other cases, where the left-hand side may depend on $\epsilon$ in other ways. The probabilistic bound scales as $1/n$ with the sample size with a proportionality constant that depends on $\delta$. 

\paragraph*{Bounded Loss Functions}

As the binomial formula is clumsy to work with and as one may want to use more general (bounded) loss functions, it is important to come up with appropriate bounds on the generalization gap 


% Thus assume $E_S(f) = \rho$, what can we say about $ 

\newpage

\bibliography{th}
\bibliographystyle{acm}

\end{document}


\section{Preliminaries}

\subsection{Large Deviation Bounds}

The most basic of all large deviation bounds is Markov's inequality.
\begin{theorem}[Markov's Inequality] 
Let $X \ge 0$, $\lambda>0$, then
\begin{align*}
P(X \ge \lambda) \le \frac{\E X}{\lambda}\,.
\end{align}
\begin{proof}
Taking expectations of $\lambda \mathbb I(X \ge \lambda) \le X$.
\end{proof}
\end{theorem}
\begin{corollary}
Let $\phi$ be any non-decreasing, non-negative function such that $\phi(\lambda)>0$, then 
 $$ 
\end{corollary}


\subsection{Loss Functions and Hypothesis Classes}

Let $\ell$ be a $L$-bounded loss function. Let $\mc H$ be a finite class of hypothesis, inducing a finite class of loss functions $\mc F$ via 


\subsection{Pointwise Bounds}

For pointwise bounds, we pick any element $f \in \mc F$ and establish bounds on $\E[f]$ in terms of $\E_S[f]$. 



\section{PAC Bayesian}

\subsection{Single Hypothesis} 


\subsection{Finite Hypothesis Classes}

\begin{theorem}
For any $\delta>0, \lambda > \tfrac 12$, and prior $p$ over $\mc F$: with probability at least $1-\delta$ over realizations of $n$-samples, the following bound holds simultaneously for all $L$-bounded $f \in \mc F$:
\begin{align*}
\E[f] \le \frac{1}{1-\frac 1{2\lambda}} \left[ 
\E_S[f] - \frac{\lambda L}{n} \left( \ln p(h) + \ln \delta \right) 
\right]
\end{align*}
\begin{proof}
\end{proof}
\end{theorem}




\section{Information Theory of Learning}

Goal of supervised learning: approximate $p(y|x)$. Use model $p_\theta(y|x)$. How sensitive with regard to parameter perturbations: Fisher information 
\begin{align}
I(\theta) = \E_x \E_{y \sim p_\theta(\cdot|x)} \left[ \nabla \log p_\theta(y|x) \nabla \log p_\theta(y|x)^\top \right] 
= - \E\left[ \mb H(y|x) \right]
\end{align}
Then one gets 
\begin{align}
\E_x\left[ \text{KL} ( p_\theta(y|x) \| p_{\theta + \triangle}(y|x)  )\right]  = I(\theta) \triangle + \mb o(\| \triangle\|^2)
\end{align}
Shannon information 
\begin{align}
I(x;z) = \E_{x} \left[ \text{KL}(p(z|x) || p(z) ) \right] 
\end{align}

\section{Introduction}

\subsection{Empirical Risk Minimization}

Let $D$ be a data distribution over input-output pairs $z = (x,y) \in  \mathbb X \times \mathbb Y$ and $\ell$ be a bounded loss function $\ell(\hat y, y) \in [0;1]$ between predicted and true output, the special case being 0/1 losses where $l(\hat y,y) \in \{0,1\}$.  A  sample $S = (z_1,\dots,z_n) \sim D^n$ is given on the basis of which we want to identify a function $g: \mathbb X \to \mathbb Y$ within a function class  $\mc G$ such that $\E[ \ell(g(x),y)]$ is small. In order to simplify notation we will define $\mc F = \{ f: f(x,y) =\ell(g(x),y), \; g\in \mc G\}$  as the family of associated loss functions and then mainly focus directly on those, as we are concerned with errors rates and losses, but not the actual predictions. 
%
In empirical risk minimization we  take the sample loss as the foundation of inference 
\begin{align}
\hat \E_S[f] := \frac 1n \sum_{i=1}^n f(z_i), \quad  f^*(S) \stackrel{\text{argmin}}\longrightarrow \{ \hat E_S[f]: f \in \mc F\}
\end{align}
It is crucial to understand what guarantees a measured empirical loss can give us for the expected loss $\E[f]$.  Let us thus define the crucial quantities that we want to control.
\begin{definition} [Generalization Gap]
The generalization gap of a function $f$ and a function class $\funct$, respectively, on a sample $S$  is defined as 
\begin{align*}
\Delta_f(S) :=  \E[f]  - \hat \E_S[f] \quad \text{and} \quad \Delta_\funct(S) := \sup_{f \in \funct} \Delta_f(S).
\end{align*}
\end{definition}
\noindent Note that gap is defined in a one-sided manner as we usually are not concerned with situations, where the expected loss is even lower than the empirical one. 

\subsection{Elementary Bounds}

We start with a simple motivating example.

\begin{example} 
Consider a single zero/one loss function $f$.  Assume that $\E[f] = \epsilon$. The the probability of  making $k$ mistakes on an $n$-sample are simply given by the binomial law
\begin{align}
\Pr(\hat E_S[f] = k/n) = \binom{n}{k} \epsilon^{k} (1-\epsilon)^{n-k},
\end{align}
so in particular the probability of zero training error can be bounded by
\begin{align}
\label{eq:basic-eps}
\Pr(\hat \E_S[f] =0) = (1-\epsilon)^n = e^{n \log (1-\epsilon)} \le e^{-n \epsilon}
\end{align}
via Taylor approximation truncated at order $\epsilon$.  We can now change the perspective and ask the following: if we were to observe that $f$ has zero empirical loss, with what confidence, i.e.~probability over draws of $S$, can we establish that its expected loss is less than $\epsilon$? Setting the right hand side in Eq.~\eqref{eq:basic-eps} to $\delta$ and get 
\begin{align}
e^{-\epsilon n} = \delta \iff \epsilon = \ln(1/\delta)/n, \quad \text{and thus} \quad 
\E[f] \le \frac{\ln(1/\delta)}{n} \;\; \text{with confidence $(1-\delta)$}\,.
\end{align}
\end{example}
\noindent The  condition of zero empirical loss in the example above is too restrictive as is the focus on 0/1 losses. A suitable large deviation bound to deal with more general cases is Hoeffdings inequality.
\begin{lemma}[Hoeffding's Inequality, simplified]
\label{lemma:hoeffding}
Consider independent $X_i \in [0;b]$, $Z := \sum_{i=1}^n X_i$,  then 
\begin{align*}
 \Pr(Z -\E Z \ge n \epsilon) \le \exp\left[  -\frac{2n \epsilon^2}{b^2} \right]
\end{align*}
\end{lemma}
\noindent Note that we get a slightly worse dependency on $\epsilon$ compared to the special case discussed above. We want to use this to get a bound on $\Delta_f$, which is straightforward:
\begin{proposition}
For all $1$-bounded loss functions $f$
\begin{align*}
\Pr(\Delta_f \ge \epsilon) \le e^{-2n \epsilon^2}, \quad \text{and} \quad \Delta_f \le  \sqrt{\frac{\ln(1/\delta)}{2n}}
\;\;\text{with confidence $(1-\delta)$.}
\end{align*}
\begin{proof} Via application of Lemma \ref{lemma:hoeffding}. \end{proof}
\end{proposition}
\noindent This is a very satisfying bound for the generalization gap of an individual $f$, but how to we extend this to control $\Delta_{\funct}$?  An elementary case is the one  where $\funct$ is finite. 
\begin{corollary} Let $\funct$ be a finite function class of $1$-bounded loss functions, then
\begin{align*}
\forall f \in \funct: P(\Delta_f \ge \epsilon) \le \Omega(\epsilon) \; \Longrightarrow \; P(\Delta_\funct \ge \epsilon) \le |\funct| \, \Omega(\epsilon)
\end{align*}
\begin{proof}
By virtue of the  union bound (aka.~Boole's inequality).
\end{proof}
\end{corollary}
The key question now is what to do for infinite $\funct$? One powerful framework is the one of Rademacher complexity discusses in the next section.


\section{Rademacher Complexity}

\subsection{Definition and Relevance}

We are interested in bounding $\Delta_\funct$ with high confidence for infinite function classes. As a first step, we aim to bound the deviations from its mean. Note that we cannot apply Hoeffding's inequality, as the conditions are too restrictive and not met. It turns out, that the following property is a fruitful generalization 
\begin{definition}[Sample Stability]
A function $\Phi$ is $\epsilon$-sample stable, if for any $n$-samples $S, S'$  differing only by one element, we have 
\begin{align*}
| \Phi(S) - \Phi(S') | \le \frac \epsilon n
\end{align*}
\end{definition}
\noindent This does apply to the generalization gap as shown in this lemma.
\begin{lemma}
\label{basic:lemma}
Let $\funct$ be a  $1$-bounded function class, then $\Delta_\funct$ is $1$-sample stable.
\begin{proof}
\begin{align*}
|\Delta_{\funct}(S')  - \Delta_{\funct}(S) |\le \sup_{\funct} \left| \Delta_{f}(S') - \Delta_f(S) \right| = \sup_{\funct}  \frac{|f(x')-f(x)|}{n} \le \frac 1n 
\end{align*}
% \begin{proof} Straightforward. \end{proof}
\end{proof} 
\end{lemma}
The sample stability is a property that \textit{a priori} puts no explicit constraints on the functional form on the function of a sample. Yet, it is a key condition to derive a large deviation bound with exponential decay in $n$: McDiarmid's inequality. 
\begin{theorem}[McDiarmid's Inequality, simplified]
\label{th:McDiarmid}
If a function $\Phi$ satisfies $|\Phi(S) - \Phi(S')| \le \tfrac 1n$ for samples $S,S' \sim D^n$ differering in one element, then 
\begin{align*}
& \Pr \left\{ \Phi(S) - \E_{S'} [\Phi(S')]  \ge \epsilon \right\} \le e^{-2 \epsilon^2 n }
\quad  \text{and} \quad  \Pr \left\{ \Phi(S) -  \E_{S'}[\Phi(S')] \le  -\epsilon \right\} \le e^{-2 \epsilon^2 n}\,.
\end{align*}
\end{theorem}
\begin{corollary}
\label{corrolary:delta} Let $S \sim D^n$, $\delta >0$, $\mc F$ a class of $1$-bounded functions, then 
\begin{align*}
\Pr( \Delta_{\mc F} - \E[ \Delta_{\mc F}] \ge \epsilon) \le e^{-2n \epsilon^2}, 
\quad \text{and w/prob.~$(1-\delta)$}
\quad \Delta_{\mc F}(S) \le \E[ \Delta_{\mc F}] + \sqrt{\frac{\log(1/\delta)}{2n}}
\end{align*}
\end{corollary}
\noindent The above corrolary allows us to bound $\Delta_{\mc F}$ on a given (noisy) random sample by its average over random samples. This is necessary as the given sample may be atypical.



We are now aiming to bound $\E_S[ \Delta_\funct]$ in terms of quantities that do not require to have access to the unknown distribution $D$.  The basic idea is to introduce expectations over two samples between which data points can be swapped via Rademacher variables. This sign randomization or symmetrization allows to approximate a difference between two expectations by an upper bound on each.

\begin{definition}[Empirical Rademacher Complexity]
Given a function class $\funct$ and a sample $S=(z_1,\dots,z_n)$. The empirical Rademacher complexity is defined as
\begin{align*}
\rade_\funct(S) := \E_\sigma \left[ \sup_{f \in \funct}  \frac 1n \sum_{i=1}^n \sigma_i f(z_i) \right], \quad \E_\sigma[ \cdot ] := \frac{1}{2^n} \! \sum_{\sigma \in \{-1,1\}^n} [\cdot]\,.
\end{align*}
\end{definition} 
\begin{definition}[Randemacher Complexity]
The Rademacher complexity of $\funct$ with regard to samples $S \sim D^n$ is defined as $\rade_{\funct}(n)   = \E_S \left[ \rade_\funct\right]$.
\end{definition}
The significance of these definition is clear from the following theorem.
\begin{theorem}[Rademacher Sample Bound]
\label{lemma:rademacher}
$$\E_S[\Delta_\funct]  \le 2\rade_{\funct}(n)$$
\begin{proof}
\begin{align*}
\E_S[\Delta_\funct] 
& 
= \E_S  \sup_{f \in \funct} ( \E[f] - \hat \E_S[f] ) 
= \E_S  \sup_{f \in \funct} \E_{S'} \left( \hat \E_{S'}[f] - \hat \E_S[f] \right) 
\\ &  \le \E_{S,S'} \sup_{f \in \funct}  \left( \hat \E_{S'}[f] - \hat \E_S[f] \right)  
 = \E_{S,S'} \sup_{f \in \funct}  \frac 1n \sum_{i=1}^n ( f(z_i') - f(z_i) )  
 \\ & 
 = \E_{S,S'} \E_\sigma \sup_{f \in \funct} \frac 1n \sum_{i=1}^n \sigma_i ( f(z_i') - f(z_i) ) 
\le 2\E_S \E_\sigma \sup_{f \in \funct} \frac 1n \sum_{i=1}^n \sigma_i  f(z_i)
\end{align*}
Note that whenever $\sigma_i=1$ terms remain unchanged, whereas for $\sigma_i=-1$ samples  $z_i$ and $z_i'$ are effectively swapped, which is of no effect as we take expectations over both samples with the same distribution. The last inequality is due to the sub-additivity of the supremum function and the fact that $\E_\sigma[\cdot] = \E_{-\sigma}[\cdot]$. 
\end{proof} 
\end{theorem}
This immediately leads to the corollary:
\begin{corollary}
$$\Delta_{\mc F}(S) \le 2 \rade_\funct(n) + \sqrt{\frac{\log(1/\delta)}{2n}} $$
\begin{proof}
\end{proof}
\end{corollary}
Finally, while is some cases one may have analytical means of bounding the Rademacher complexity, it is possible to work with the empirical Rademacher complexity, for which:
\begin{lemma}
$$\rade_{\funct}(n) \le \rade_{\funct}(S) + \sqrt{\frac{\log(1/\delta)}{2n}} $$
\begin{proof} By McDiarmid's inequality. \end{proof}
\end{lemma}
With this lemma and the union bound, one immediately gets the following:
\begin{corollary}
$$\E[\funct] - \hat\E_S[\funct]\le  2 \rade_{\funct}(S) +  3\sqrt{\frac{\log(2/\delta)}{2n}}$$
\end{corollary}
What have we accomplished? Essentially, we have bound the generalization gap by purely empiricial quanties. Yet, one of the computational difficulties that remains is the computation of the empirical Rademacher complexity, which potentially may require to perform an exponential sum over the realizations of the Rademacher variables $\sigma$.

\section{Covering Numbers}

\subsection{Definitions} 

As we have seen in the finite $\funct$-case, bounding the cardinality of the hypothesis set yields uniform bounds through applications of the union bound. The idea of $\epsilon$-covers amd covering numbers is to reduce the general case to the finite one. 
\begin{definition}[Cover, Covering Number]
Given a set $U$ and an arbitrary norm $\| \cdot \|$, a finite $V \subseteq U$ is an $\epsilon$-cover, if 
\begin{align*}
\sup_{A \in U} \min_{B \in V} \| A - B\| \le \epsilon
\end{align*}
The covering number of $U$ is the defined as
\begin{align*}
\mathcal N_\epsilon(U) = \min\{ |V|: \text{$V$ is an $\epsilon$-cover of $U$} \}
\end{align*}
\end{definition}

\subsection{Covers for DNNs}

We follow the approach advocated in \cite{bartlett2017spectrally} to look at set of activiations that can be generated from a fixed data matrix $\mb X =(\mb x_1\dots \mb x_n)\in \Re^{d\times n}$. We model a DNN  as a sequence of maps $G = F^l \circ \dots \circ F^1$, where each $F^i$ is parameterized by $\theta^i \in \Theta^i$. Let us look at a generic layer-to-layer transfer function $F$ with parameter $\theta \in \Theta$, then 
\begin{align}
\mc X^{+} = \bigcup_{\mb X \in \mc X} F(\mb X; \Theta), \quad F(\mb X; \Theta) := \{ F(\mb X;\theta): \; \theta \in \Theta\}
\end{align}
denotes the set of possible next-layer activities. Clearly, strarting from $\mc X^0=\{\mb X\}$ we can generate a sequence $\mc X^1, \dots, \mc X^L$. We would like to bound the covering number $\mc N_\epsilon(\mc X^{l})$. We will also generate finite (i.e.~quantized) sets or codebooks $\mc Y^1\dots, \mc Y^l$ recursively from $\mc Y^0=\{\mb X\}$ as follows.
\begin{align}
\label{eq:propagated-quantization}
\mc Y^{+} = \textnormal{quantize}\left(\bigcup_{{\mb Y} \in {\mc Y}} F(\mb Y; \Theta)\right)\,.
\end{align}
This means, we propgate only the codebooks forward to the next level and quantize the $F$-image into a new codebook.
\begin{lemma}
Assume that a sequence of codebooks $\mc Y^1,\dots,\mc Y^l$ is generated via propagated-quantization 
\begin{align*}
\mc Y^{+} = \bigcup_{{\mb Y} \in {\mc Y}} \textnormal{$\epsilon$-mincov}(F(\mb Y; \Theta)) \,.
\end{align*}
for some sequence of $\epsilon^1,\dots,\epsilon^l$ and arbitrary choices of the minimal covers.  Then 
\begin{align*}
\text{(1)}: & \quad |\mc Y^l| \le \prod_{i=1}^{l} \sup_{\mb Y \in \mc Y^{i-1}} \mc N_{\epsilon^{i}}(F(\mb Y; \Theta))
\\
\text{(2)}: & \quad \mc Y^l \text{ is a $\delta$-cover of $\mc X^l$ with } \delta =  \sum_{i=1}^l \epsilon^i \cdot \textnormal{Lip}(F^i \circ \dots \circ F^1)
\end{align*}
\begin{proof}
The cardinality statement (1) follows easily by induction as $| \mc Y^0|=1$ and 
\begin{align*}
|\mc Y^+| \le | \mc Y| \cdot  \sup_{\mb Y \in \mc Y} \mc N_{\epsilon}(F(\mb Y; \Theta))\,.
\end{align*}
To show the cover accuracy statement (2), pick $\mb X^l \in \mc X^l$ with pre-image sequence $\mb X^{l-1}, \dots, \mb X^{1}, \mb X^0$. Consider the  quantized sequence $\mb X^0 = \hat{\mb X^0}, \hat {\mb X}^1,\dots, \hat {\mb X}^l$ generated by 
\begin{align*}
\hat {\mb X}^+  = \arg\min_{\mc Y^+} \| \mb Y^+ - F(\hat{\mb X}) \|, \quad \text{where} \quad \| \hat {\mb X}^+  - F(\hat{\mb X}) \| \le \epsilon
\end{align*}
For each $\hat {\mb X}^i$  define  the unquantized contiuation 
\begin{align*}
\hat {\mb X}^{i \to l} = (F^l \circ \dots \circ F^{i+1})(\hat {\mb X}^i)
\end{align*}
As $\hat {\mb X}^{0 \to l} = \mb X^l$ and $\hat{\mb X}^{l \to l}  = \hat {\mb X}^l$ the triangle inequality yields
\begin{align*}
\| \mb X^l - \hat {\mb X}^l \| \le \sum_{i=1}^l \| \hat{\mb X}^{i \to l} -  \hat{\mb X}^{(i-1) \to l}\| \le \sum_{i=1}^l \epsilon^i \cdot \textnormal{Lip}(F^i \circ \dots \circ F^1)
\end{align*}
as $\hat{\mb X}^{(i-1) \to l}=  (F^l \circ \dots \circ F^{i+1})F^i(\hat {\mb X}^{i-1})$ whereas 
$\hat{\mb X}^{i \to l}=  (F^l \circ \dots \circ F^{i+1})(\hat {\mb X}^i)$.% and $\| F^i(\hat {\mb X}^{i-1} - \hat {\mb X}^i \| \le \epsilon^i$. 
\end{proof} 
\end{lemma}
The advantage of the above lemma is that we only need to estimate Lipschitz constants on network transfer functions (or upper bounds thereof) as well as covering numbers for images of singleleton sets (i.e.~generated from atoms $\hat {\mb X}$ via $F(\hat {\mb X})$). 

\subsection{Layer-to-Layer Covering Numbers}

\begin{lemma}[Maurey's Sparsification]
Given a Hilbert space $(\mc H, \| \cdot \|)$ and  a representaton $U = \beta \sum_{i=1}^m \alpha_i V_i$, where $U, V_i \in \mc H$, $\alpha_i \ge 0$, $\sum_i \alpha_i =1$. For any positive integer $k$ there exist multiplicities $(k_1,\dots,k_m)$, $\sum_i k_i=k$ such that 
\begin{align*}
\left\|   U - \frac{1}{k} \sum_{i} k_i V_i  \right\|^2 
 \le \frac{1}{k} \sum_i \alpha_i \| V_i \|^2 
 \le \frac{1}{k} \max_i \| V_i \|^2 
\end{align*}
\begin{proof} 
Define a random variable $I \in [1;n]$ via $\Pr(I=i) = \alpha_i$. Then
\begin{align*}
U - \E[V_I] =0 \quad \text{and} \quad \E[(V_I - U )^2] = \E[V_I^2] - U^2  \le  \E[V_I^2]  = \sum_i \alpha_i \| V_i\|^2
\end{align*}
For an iid $k$-sample average, the variance is decreased by a factor $1/k$. As the first inequality holds in expectation, there also needs to be a realization of choices, which can be encoded in the multiplicities. The final inequality follows trivially. 
\end{proof}
\end{lemma}


\newpage


, $\theta = (\theta^1,\dots,\theta^L) \in \Theta$. We assume $\Theta$ to be compact, so that it allows for a finite cover.  Note that once $\Theta^l$ are fixed, the DNN recursively defines  a set of activations at each layer

\begin{definition}[Propagated Cover] 
For any choice of $\epsilon^1,\dots,\epsilon^L$, a propagated  sequence of covers $\mc F^1, \dots \mc F^L$ obeys the following property:
\begin{align*}
\mc F^{l+1} \in  \textnormal{$\epsilon^{l+1}$-mincover} \bigcup_{\theta^l \in \Theta^l} F^l(\mc F^l; \theta^l)
\end{align*}
\end{definition}
We can think of a propagated cover in the following way. For fixed parameters $\theta$ we propogate sample activities by quantizing the activity matrix at $\mb X^l$ with the codebook $\mc F^l$, $\mb X^l \mapsto \hat{\mb X}^l$ and propagating $\hat{\mb X}^l$  instead of $\mb X^l$ to the next layer.

\begin{lemma}
Let $(F^1,\dots,F^L)$ be a propagated $(\epsilon^1, \dots, \epsilon^L)$-cover, then
\begin{align*}
\mc F^L \in \textnormal{$\delta$-cover}(\mc X^L), \quad \delta = \sum_{l=1}^L \epsilon^l \cdot \textnormal{Lip}(F^l \circ \dots \circ F^1)
\end{align*}
\begin{proof} 
For each $\mb X^L$ there is a parameter $\theta$ such that $\mb X^L = F(\mb X; \theta)$ with intermediate layer activities $\mb X^l$. 
\end{proof}
\end{lemma}



\newpage

Moreover, we assume each $F^l(\cdot; \theta^l \in \Theta^l)$ is $\rho^l$-Lipschitz.  


\newpage



 by varying paraneters


A generic step consists in applying a Lipschitz-transformation $\Pi$ to a set of matrices $\mc X$.
\begin{lemma}
Let  $\mc F $  be an $\epsilon$-cover of  $\mc X$. Assume $\Pi$ is $L$-Lipschitz. Then for any $\delta>0$
\begin{align*}
\mc N_{L\epsilon + \delta}(\Pi(\mc X)) \le |\mc F| \cdot \sup_{\mb X \in \mc X} \mc N_\delta( \Pi(\mb X)) \,.
\end{align*}
\begin{proof}
For each  $\mb X_0 \in \mc F$ consider a minimal $\delta$-cover for $\Pi(\mb X_0)$  and their union
\begin{align}
\mc F^+ := \bigcup_{\mb X_0 \in \mc F} \text{min-$\delta$-cover}(\Pi(\mb X_0)) 
\end{align}
Obviously in terms of cardinality $| \mc F^+| \le |\mc F| \cdot \sup_{\mb X } \mc N_\delta(\Pi(\mb X))$. Note that  each  $\mb Z \in \Pi(\mc X)$ has a pre-image $\mb X$ for which  $\mb X_0 \in \mc F$ such that $\| \mb X - \mb X_0\| \le \epsilon$. Consider $\mb Z_0 = \Pi(\mb X_0)$ for which the Lipschitz condition on $\Pi$ implies $\| \mb Z - \mb Z_0\| \le L \epsilon$. Moreover, there is a $\tilde{\mb Z}_0 \in \mc F^+$ such that $\| \mb Z_0 - \tilde{\mb Z}_0 \| \le \delta$. By the triangle inequality, $\|\mb Z - \tilde{\mb Z}_0\| \le L\epsilon + \delta$.
\end{proof}
\end{lemma}


% Note that a DNN consists in a sequence of transformations, where pre-activations at a subsequent layer are typically given via $\mb X^+ = \mb A \sigma(\mb X)$ with $\mb A \in \mc A$.  
%\begin{definition}[Pre-Activation Propagation]
%\begin{align*}
%\Pi^\sigma_{\mc A}(\mb X) = \{ \mb A\sigma(\mb X): \;  \mb A \in \mc A\}
%\end{align*}
%\end{definition}


\begin{corollary} 
A DNN $F$ of depth $m$ can be written as $F = F_m \circ \dots \circ F_1$. Each $F_i$ induces a matrix-to-matrix activity mapping $\Pi_i: \mb X_{i-1} \mapsto \mb X_i$ by applying $F_i$ column-wise on the per sample activation vectors. Assume $\Pi_i$ is $L_i$-Lipschitz and pick a sequence $\epsilon_1,\dots,\epsilon_m$, then 
\begin{align*}
\mc N_\delta(\Pi(\mb X)) \le \prod_{i=1}^m \sup \left\{ \mc N_{\epsilon_i}(\mb Z) : \mb Z \in (\Pi_i \circ \dots \circ \Pi_1)(\mb X) \right\}, \quad \delta = \sum_{i=1}^m  \epsilon_i \left( \prod_{j>i} L_j \right)
\end{align*}
\end{corollary}

\section{PAC Bayes Bounds}

\bibliography{th}
\bibliographystyle{acm}

\end{document}

\end{document}


\item 
\item Margin bounds.  One can trade-off the empirical loss term vs.~the Rademacher complexity by modifying the function class (of loss functions). Define the following $1$-bounded $\gamma$-margin versions of functions 
\begin{align}
m_\gamma(t) = \begin{cases} 0 & t < -\gamma \\ 1 + t/\gamma & -\gamma \le t \le 0 \\ 1 & t>0\end{cases}, \quad \funct_\gamma =  m_\gamma \circ \funct
\end{align}
Note thet $m_\gamma(t)  \ge \text{sign}(t)$ and this can be used as an upper bound on the 0/1 classification error.  The details of how to use this for multiclass problems can be found in \cite{bartlett2017spectrally}, which leads to \cite[Lemma 3.1]{bartlett2017spectrally}
\begin{align}
\E[f_0]  \le \hat \E_S[f_\gamma] + 2 \rade_{\funct_\gamma}(S) +  3\sqrt{\frac{\log(2/\delta)}{2n}}
\end{align}
\end{enumerate}

\section{Covering Numbers}
\begin{enumerate}
% 
\item 
% 
We would like to understand how we can construct covers for $\Pi^\sigma_a(\mc X)$ from covers for $\mc X$. 
\begin{lemma}
\end{lemma}
\item Maurey sparsification lemma\\
\begin{align}
\text{starting point} \quad & U = \beta \sum_{i} \alpha_i V, \quad \sum_{i} \alpha_i =1, \; \alpha_i \ge 0  \\
\text{random rounding} \quad & \Pr[W = \beta V_i] = \alpha_i, \quad \E[W] = U \\
\text{variance} \quad & \E \| U - W\|^2 
\end{align}
\item Lemma 3.2\\

\newpage

\item 
advocates an approach to compute covering numbers for matrices. Assume that $\mb X$ is the $n \times d$ data matrix with $\tfrac 1n \| \mb X\|^2_F =\tfrac 1n \sum_{i,j} x_{ij}^2 = R^2$. We construct pre-activation matrix sets with bounded norm matrices as follows:

\newpage




\newpage

% 
\item \cite{bartlett2017spectrally} advocates an approach to compute covering numbers for matrices. Assume that $\mb X$ is the $n \times d$ data matrix with $\| \mb X\|^2_F =nR^2$. The function family under consideration is 
\begin{align}
\mathcal A(a) := \{ \mb X \mb A : \mb A \in \Re^{d \times m}, \| \mb A\|_{2,1}\le a \}
\end{align}
with covers relative to the Frobenius norm.
\begin{theorem}[Bartlett et al, 2017]
$$\ln \mathcal N(\mathcal A_s(a), \epsilon, \| \cdot\|_F) \le \left\lceil \frac{a^2b^2 m^{2/r}}{\epsilon^2} \right\rceil \ln(2dm), \quad r = (1 -1/s)^{-1} $$
\begin{proof}
Relies Maurey sparsification lemma (Pisier, 1980).
\end{proof}
\end{theorem}
\item We simplify \cite[Theorem 3.3]{bartlett2017spectrally}. Consider networks with the following architecture. 
\begin{itemize}
\item $L$ layers with $1$-Lipschitz activation functions $\sigma_l$ for which $\sigma_l(0)=0$. 
\item Weight matrices $A_l$ such that $\| A_l\|_2 = s_l$ and norm bounds  $\| A_l^\top\|_{2,1} = b_l$.
\item Maximum layer width $W$, data matrix norm $\| \mb X\|_F^2 = nR^2$
\item Matrix function class $\mathcal F_{\mathbf X} = \{ \mb X_{L+1}: \text{ where  } \mb X_{l+1} = \sigma_l(\mb A_l \mb X_l), \mb X_1=\mb X\}$

\end{itemize}
\begin{theorem}
$$\mathcal N(\mathcal F_{\mathbf X},\epsilon, \|\cdot\|_F) \le \frac{nR^2 \ln(2W^2)}{\epsilon^2} \left(\prod_{l=1}^L s_l^2\right) \left( \sum_{l=1}^L \left[ \frac{b_l}{s_l} \right]^{\frac 23}\right)^3$$
\end{theorem}
\end{enumerate}


\newpage



\noindent We will construct covers of pre-activation matrices of layers $l=1,\dots,L$. Let us define
\begin{definition}[Layer Pre-Activation Matrices]  Given the data matrix $\mb X$. Let $\| \cdot \|$ be any matrix norm. For any choice of norm-bounded weight matrices $\mb A_{1:l} := \mb A_1,\dots, \mb A_{l}$ with $\| \mb A_i\| \le a_i$, define the set of admissible pre-activaton matrices via\footnote{Implicit dependencies on $\{ a_i\}$ and $\mb X$ have been dropped.}
\begin{align}
\mathcal G_{l+1}(\mb A_{1:l}) := \{\mb  A_{l+1} \mb X_{l} (\mb A_{1:l}): \| \mb A_{l+1} \| \le a_{l+1}\},  
\quad \mathcal G_{l+1} := \{ \mathcal G_{l+1}(\mb A_{1:l}) : \| \mb A_i\| \le a_i (\forall i) \}
\end{align}
where $\mb X_{l}(\mb A_{1:l})$ is the activation matrix of layer $l$, i.e.~recursively $\mb X_l = \sigma_l(\mb A_l \mb X_{l-1})$, $\mb X_1 = \mb X$.
\end{definition}
\begin{lemma}
% Assume that we have an $\epsilon_l$-cover $\mathcal F_l$ of $\mathcal G_l$, then 
\begin{align*}
\mathcal N(\mathcal G_{l+1}, \epsilon_{l+1}, \| \cdot \|)  \le 
\mathcal N(\mathcal G_{l}, \epsilon_{l}, \| \cdot \|)  \cdot \sup_{\mb A_{1:l-1}} \mathcal N(\mathcal G_{l}(\mb A_{1:l-1}) , \epsilon_l, \|\cdot\|)
\end{align*}
\begin{proof}
Assume that we have a minimal $\epsilon_l$-cover $\mathcal F_l$ of $\mathcal G_l$ and chose $\mb F \in \mathcal F_l$. As $\mb F \in \mathcal G_l$ it can be represented as $\mb F = \mb A^{\mb F}_l  \mb X_{l-1} (\mb A^{\mb F}_{1:l-1})$  for some choice of norm-bounded matrices $\mb A^{\mb F}_{1:l}$. Now, construct a minimal $\epsilon_{l+1}$ cover $\mathcal F_{l+1}(\mb F)$ of 
\begin{align}
\{ \mb A_{l+1} \sigma(\mb F): \| \mb A_{l+1}\| \le a_{l+1} \} = \mathcal G_{l+1}(\mb A^{\mb F}_{1:l})
\end{align}
for which we get the cardinality bound 
\begin{align}
|\mathcal F_{l+1}(\mb F) |\le \mathcal N \left(\mathcal G_{l+1}(\mb A^{\mb F}_{1:l}) , \epsilon_{l+1},\|\cdot \|\right) \le 
\sup_{\mb A_{1:l-1}} \mathcal N(\mathcal G_{l}(\mb A_{1:l-1}) , \epsilon_l, \|\cdot\|)
\end{align}
which has a cardinality 
\end{proof}
\end{lemma}


Assume that weight matrices $\mb A_{1:l-1}$ have been fixed obeying $\| \mb A_i \|_F \le a_i$ ($\forall i$). Moreover assume that a cover 


We are interested in covering
\begin{align}
\mathcal F_l  \stackrel{\text{$\epsilon_l$-cover}}\longrightarrow \mathcal G_{l}(\mb A_{1:l-1}) := \{\mb  A_l F(\mb A_{1:l-1},\mb X): \| \mb A_l \|_F \le a_l\}
\end{align}
which is the set of possible pre-activation matrices at layer $l$, given the data matrix and fixed choices for upstream weight matrices. Clearly 
\begin{align}
| \mathcal F_l | \le \sup_{\mb A_{1:l-1}} \mathcal N(\mathcal G_{l}(\mb A_{1:l-1}) , \epsilon_l, \|\cdot\|_F)
\end{align}



Starting with some matrix $\mb X$, assume we have a minimal-$\epsilon_1$ cover $\mathcal F_1$ for $\mathcal A_1^{id} = \{ \mb A_1 \mb X: \| \mb A_1\|_F \le a_1\}$. For every $F \in \mathcal F$ construct an $\epsilon_2$-cover $\mathcal F_2$ for 
\begin{align}
\mathcal A_2^\sigma = \{ \mb A_2 \, \sigma(\mb F), \| \mb A_2 \|_F \le a_2 \} 
\end{align}


\noindent 



\newpage


\noindent The following lemma is simple, but important.


\begin{theorem}[Rademacher Generalization Bound]
Let $\funct$ be a family of functions $f: \mathcal X \to [0;1]$. Then for any $\delta>0$ with probability $(1-\delta)$ over the draw of an $n$-sample $S$: For all $f \in \funct$:
\begin{align*}
\E_D [g(x)] & \le \E_S[g(x)] + 2 \rade_n(\funct) + \sqrt{\frac{\log(1/\delta)}{2n}} \quad & (1) \\
\E_D [g(x)] & \le \E_S[g(x)] + 2 \rade_S(\funct) + 3\sqrt{\frac{\log(2/\delta)}{2n}} \quad & (2)\\
\end{align*} 
\begin{proof} 
\end{proof}
\end{theorem}

\bibliography{th}
\bibliographystyle{acm}

\end{document}



Straightforward calculations yield: 
\begin{theorem}
\begin{align*}
\forall f \in \mc F: \; \Pr(\Delta_f \ge \epsilon) \le e^{-2n \epsilon^2}, \quad  \text{and} \quad \Pr( \Delta_{\mc F} \ge \epsilon ) \le |\mc F| e^{-2n \epsilon^2}
\end{align*}
and with probabilty $(1-\delta)$  over the sample $S$
\begin{align*}
 \Delta_{\mc F} \le    \sqrt{\frac{\ln |\mc F| + \ln (1/\delta)}{2n}}
\end{align*}
\begin{proof}
For bounded loss function $b_i-a_i=1$, so that $\sum_i (b_i-a_i)^2 = n$, then setting $t=n\epsilon$. Applying the union bound gives the second claim and solving for $\epsilon$ as a function of $\delta$ the third.
\end{proof}
\end{theorem}

to control for the generalization gap. 

\subsection{Finite Function Classes}

Let us discuss the special case of finite $\funct$. First, assume that $\hat \E_S[f]=0$, then we can bound the generalization gap  by elementary means. Note that in this case $\Delta_f= \E[f]$. 
%
\begin{theorem}  Let $\mc F$ be a finite set of zero-one loss functions and $\mc F_\epsilon := \{ f\in \mc F: \E[f] \ge \epsilon\}$.
\begin{align*}
\Pr(\exists f \in \funct_\epsilon: \hat \E_S[f] =0)  \le |\funct| e^{-\epsilon n}
\end{align*}
\begin{proof} 
\begin{align*}
\forall f \in \funct_\epsilon: \; \Pr (\hat \E_S[f]=0) \le (1-\epsilon)^n = e^{n \log (1-\epsilon)} \le e^{-\epsilon n}
\end{align*}
The claim then follows from the union bound.
\end{proof} 
\end{theorem}  
\begin{corollary}
With probability $(1-\delta)$ over $S \sim D^n$
\begin{align}
\forall f:  \hat \E_S[f] =0 \; \Longrightarrow \E[f]  \le \frac{\ln |\mc F| + \ln (1/\delta)}{n}
\end{align} 
\begin{proof}
Setting the right hand side equal to $\delta$ and solving for $\epsilon$.
\end{proof}
\end{corollary}
%
In order to get rid of the condition of zero empirical loss, one can use
\begin{lemma}[Hoeffdings Bound]
\label{lemma:hoeffding}
Consider independent $X_i \in [a_i; b_i]$, $Z := \sum_i X_i$,  then 
\begin{align*}
 \Pr(Z -\E Z \ge t) \le \exp\left[\frac{-2t^2}{\sum_i (b_i-a_i)^2} \right]
\end{align*}
\end{lemma}
Straightforward calculations yield: 
\begin{theorem}
\begin{align*}
\forall f \in \mc F: \; \Pr(\Delta_f \ge \epsilon) \le e^{-2n \epsilon^2}, \quad  \text{and} \quad \Pr( \Delta_{\mc F} \ge \epsilon ) \le |\mc F| e^{-2n \epsilon^2}
\end{align*}
and with probabilty $(1-\delta)$  over the sample $S$
\begin{align*}
 \Delta_{\mc F} \le    \sqrt{\frac{\ln |\mc F| + \ln (1/\delta)}{2n}}
\end{align*}
\begin{proof}
For bounded loss function $b_i-a_i=1$, so that $\sum_i (b_i-a_i)^2 = n$, then setting $t=n\epsilon$. Applying the union bound gives the second claim and solving for $\epsilon$ as a function of $\delta$ the third.
\end{proof}
\end{theorem}

%
% REST 
%
\begin{enumerate}
\item 
\item \item With this definition we get from Lemma \ref{lemma:rademacher}
\begin{theorem} For all $[0;1]$ functions $f \in \funct$ and $\delta>0$ with probability at least $(1-\delta)$
$$\E[f] \le \hat\E_S[f] + 2 \rade_{\funct}(n) +  \sqrt{\frac{\log(1/\delta)}{2n}}$$
\begin{proof}
