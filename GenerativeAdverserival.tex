\documentclass{article}
\usepackage{amsmath}
\title{Learning Domain-Invariant Representations \\ {\small CONFIDENTIAL --  DO NOT DISTRIBUTE}}
\author{Thomas Hofmann\\ ETH Zurich}

\newcommand{\E}{{\mathbf E}}
\renewcommand{\H}{{\mathcal H}}

\begin{document}
\maketitle

\paragraph{Goals}  We are interested in the problem of domain adaptation: given labeled training data from one or more domains or tasks, we would like to maximize the predictive accuracy in a new domain, as represented by a set of unlabelled examples.  Following the seminal work of \cite{ben2007analysis,ben2010theory} and more recent deep learning approaches such as \cite{ganin2016domain}, we cast this problem as one of representation learning. Our goal thus is to learn intermediate representations that are \textit{domain-invariant}, i.e.~to extract features that are not specific to any particular domain, but that possess good predictive power across domains. We suggest to accomplish this by penalizing any implicit information in the learned representation about from what domain the data originates. 

\paragraph{Learning Generative Models} A key challenge in the development of complex generative models, e.g.~when using deep neural network architectures, is the normalization of the model. Even in relatively simple models, proper normalization can make learning intractable, for instance, because it requires the computation of a partition function. It is therefore of interest to be able to work with unnormalized models. One well-known approach of how to transform a generative objective into a discriminative one is known as \textit{noise contrastive estimation} \cite{gutmann2010noise}. Here density estimation is reduced to logistic regression in  a principled manner. 

A perhaps more practically relevant approach is generative adversarial learning \cite{goodfellow2014generative}. In this setting, one combines two mechanisms: a data generating mechanism $f_\theta: z \mapsto x$, where $z$ is a latent variable with distribution $p_z$ and $x$ a sample and a discriminator $g_\phi: x \mapsto \sigma \in [0;1]$ which outputs Bernoulli probabilities. Learning in adversarial models amounts to solving a min-max problem of the type 
\begin{align}
\label{eq:criterion}
\min_\theta \max_\phi \E_x \left[ \log g_\phi(x) \right] + \E_z \left[ \log (1-g_\phi(f_\theta(z))) \right]
\end{align}
Basically, the examples $f_\theta(z)$ generated by the generative mechanism should not diverge much from the real data $x$. This divergence is measured relative to the discrimination capabilities of the chosen function class $\{ g_\phi\}$. The algorithm proposed in \cite{goodfellow2014generative} to find a saddle-point is to perform alternating minibatch gradient ascent in $\phi$ and descent in $\theta$. As pointed out consistently, controlling the learning process is non-trivial, e.g.~\cite[Section 4.1]{li2015generative}. 

An alternative approach due to \cite{li2015generative} is to modify the GAN idea in a more principled manner by taking a  proper discrepancy function between probability distributions as a starting point. Specifically, \cite{li2015generative} makes use of the maximum mean discrepancy (MMD), which is simply the mean squared distance of a vector of statistics. Using kernels (e.g.~Gaussian), one can create a very rich set of statistics, so that MMD is quite powerful in pinpointing discrepancies between pairs of distributions. Note that MMD is differentiable \cite[Eq.~(6)]{li2015generative} and does not require additional parameters. 

The accuracy of the MMD test may deteriorate in high dimensions \cite{ramdas2015decreasing}. An improved version of the above GMMD model is one that uses an additional auto-encoder \cite[Section 4.2]{li2015generative}. The clever idea is to first train an auto-encoder to find a suitable low-dimensional code space and then to use MMD to generate code vectors, which are then further decoded by the auto-encoder.

\paragraph{Domain Invariance from Discrepancy}  Connecting the above ideas of minimizing the discrepancy between distributions with the goal of learning domain-invariant representations is quite straightforward. We can apply the discrepancy measure to the learned data representation for which we want to enforce domain-independence. Using this idea based on GAN essentially results in the model proposed in  \cite{ganin2016domain}. Conceptually, one splits a deep network into a representation learning part and a classification part and then enforces domain-invariance of the learned representation.\footnote{Practically, they are "inspired"  by the $\H$-divergence theory \cite{ben2007analysis}, but in the end  simply use a logistic likelihood, cf.~\cite[Eq.~(7)+]{ganin2016domain}, the effect of which is controlled via a hyper-parameter. Training is done via an alternating SGD variant. The justification via a gradient reversal layer is weak and the pseudo-function argument completely bogus!}

We propose to use the moment matching approach as a better grounded principle for learning domain-invariant representations. Basically, we can directly combine the two ideas: split the network into a representation learning and a prediction part, then enforce the desired invariance on the representation via the maximum mean discrepancy. Because we are not generating high dimensional data, the auto-encoder enhancement is not required. The advantage relative to  \cite{ganin2016domain} is that we do not need to learn the parameters of the discriminator in an adversarial manner (min-max problem). Rather, we work with a fixed set of statistics as implicitly defined and weighted by the chosen kernel.

\paragraph{X-Domain Learning} We can also easily extend the above idea by introducing domain-specific features. Basically we generate a domain-invariant representation as suggested before. In addition, we also allow for domain specific features, where weights are only adjusted by training over examples from the respective domain. Basically one could have $(L-1)$ layers, then the $L$-th layer, which is partitioned into a domain-invariant and domain-specific part. Then the $(L+1)$-th layer would be the prediction layer (i.e.~softmax). It would be interesting to investigate, whether the additional domain-specific units can help to obtain more meaningful domain-invariant representations. In addition, this could be a natural way of training a new domain, when at least some (small set) of training data is available. One would expect that the off-domain training data that has been used for all other weights and in particular the domain-invariant data representation provides an effective inductive bias, which makes learning in a new domain vastly more efficient. 


\bibliography{deepnets}
\bibliographystyle{acm}

\end{document}

\paragraph{Conditioning} It is sometimes interesting to control the generative process through an additional conditoning variable, e.g.~a class variable. This has been proposed under the name CGAN in \cite{mirza2014conditional}. The (small) conceptual difference here is that the side information is given to the generative mechanism and copied over to the discriminator. This approach has also been used successfully in the image domain by \cite{denton2015deep}, where the generative process uses a Laplacian pyramid to successively add detail via Laplacian edgemaps. Here, up-sampled version of low-resolution images provide the conditioning for the next step. The models at different resolutions are then trained by the standard (C)GAN procedure. 

\paragraph{Analysis}
Optimal discriminator for a given generating mechanism. Define $p_\theta(x) := \int f_\theta(x|z) dp(z)$ then 
\begin{align}
g_\phi(x) = \frac{p(x)}{p(x) + p_\theta(x)}
\end{align}
This is straightforward as Eq.~\eqref{eq:criterion} is just a binomial likelihood. The "theorem" is trivial and just states that the (only) optimum is achieved at $p_\theta = p$, i.e.~when the model distribution matches the data distribution exactly.\footnote{Proposition 2 is completely useless.} 

\paragraph{Domain Adaptation} 

$\H$-divergence \cite{ben2007analysis} relative to a hypothesis class of binary classifiers, $g \in \H$ and define $X^+_g = \{ x: g(x) =1\}$, then 
\begin{align}
D(P,Q) = 2 \sup_{g \in \H}  \left| P(X^+_g) - Q(X^+_g) \right|
\end{align}
In \cite{ganin2016domain} it is proposed to use a regularizer to enforce domain-invariance, based on the empirical version of the $\H$-divergence. Conceptually, they split a deep network into a representation learnign part and a classification part and then enforce domain-invariance of the learned representations. Practically, they are only "inspired"  by the $\H$-diverence theory and simply use a logistic likelihood, cf.~\cite[Eq.~(7)+]{ganin2016domain}, the effect of which is controlled via a hyper-parameter. Training is done via an alternating SGD variant.\footnote{The justification via a gradient reversal layer is weak and the pseuo-function argument completely bogus!}




\paragraph{Tricks of the Trade}
\begin{itemize}
\item \cite{goodfellow2014generative} proposes to initially maximize $\log g_\phi(f_\theta(z))$ with regard to $\theta$
\item \cite{goodfellow2014generative} use non-parametric density estimators (e.g.~Parzen windows) to compute likelihood for test set evaluation 
\end{itemize}

