\documentclass{article}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{mathtools}
\usepackage[margin=3cm]{geometry}
\usepackage{marginnote}
\usepackage{color}
\usepackage{xcolor}% http://ctan.org/pkg/xcolor

%%% COLORS
\definecolor{Blue}{rgb}{0.3,0.3,0.9}
\definecolor{Orange}{rgb}{0.8,0.4,0.1}
\definecolor{Red}{rgb}{0.9,0.1,0.1}
\newcommand{\up}[1]{^{(#1)}}
\newcommand{\textblue}[1]{{\color{Blue} #1}}
\newcommand{\textbblue}[1]{{\bf\color{Blue} #1}}
\newcommand{\textred}[1]{{\color{Red} #1}}
\newcommand{\textbred}[1]{{\bf\color{Red} #1}}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{define}[theorem]{Definition}

%%%
\newcommand{\pro}[2]{{\left\langle #1, #2 \right\rangle}}
\newcommand{\E}{{\mathbf E}}
\renewcommand{\P}{{\mathcal P}}
\newcommand{\G}{{\mathcal G}}
\newcommand{\D}{{\mathcal D}}
\renewcommand{\L}{{\mathcal L}}
\newcommand{\obj}{{\mathcal J}}% objective
\newcommand{\mX}{{\mathbf X}} % matrix X
\newcommand{\mU}{{\mathbf U}} % matrix U
\newcommand{\mV}{{\mathbf V}} % matrix U
\newcommand{\mZ}{{\mathbf Z}} % matrix Z
\newcommand{\mY}{{\mathbf Y}} % matrix Z
\newcommand{\mW}{{\mathbf W}} % matrix Z
\newcommand{\mXi}{{\mathbf \Xi}} % matrix Z
\newcommand{\vu}{{\mathbf u}} % vector u
\newcommand{\vz}{{\mathbf z}} % vector z
\newcommand{\vxi}{{\mathbf \xi}} % vector \xi
% \newcommand{\opt}[1]{{#1}^{{\text{\tiny opt}}}}
\newcommand{\opt}[1]{{#1}^{{\dagger}}}
\newcommand{\pri}{{\pmb \beta}}
\newcommand{\dua}{{\pmb \alpha}}
\newcommand{\mI}{{\mathbf I}}
\newcommand{\mT}{{\mathbf T}}
\newcommand{\y}{{\mathbf y}}
\newcommand{\trace}{{\text{Tr}}}
\newcommand{\x}{{\mathbf x}}
\newcommand{\z}{{\mathbf z}}
\newcommand{\vzeta}{{\pmb \zeta}}
\newcommand{\adual}{{\mathbf a}}
\newcommand{\bdual}{{\mathbf b}}
\newcommand{\w}{{\mathbf w}}
\newcommand{\W}{{\mathbf W}}
\renewcommand{\Re}{{\mathbb R}}
\newcommand{\bigO}{{\mathbf O}}
% \setlength\parindent{0pt}

\title{Generative Language Models}
\author{Thomas Hofmann}

\begin{document}

\maketitle

\paragraph{Generative Language Models and Perplexity} 
A generative language model assigns probabilities to sentences or utterances, and ideally also allows for  conditioning on contextual or situative information. Most work on statistical language models is based on the product rule of probability and assigns probabilities to word sequences via 
\begin{align}
p(\w_{1:T}) = \prod_{t=1}^T p(\w_t | \w_{1:t-1})
\label{eq:product-rule}
\end{align} 
Models are then naturally trained via likelihood maximization or equivalently perplexity minimization, which essentially measures the quality of a language model by its accuracy in quantifying the next word probabilities. Because of finite data and resources, the models one has to use are imperfect though. We may have to restrict the conditioning context as in the classical $n$-gram models or reduce contexts by mapping them to vector representations as in modern neural language models. This may lead to a lack of sentence-level coherence despite the fact that perplexities can be quite low.  Conceptually think about two ways that language models can go wrong: First a model may produce only grammatical and sensible sentences, yet may be off in terms of quantifying their probabilities. Such a model may have high next word perplexity. Second a model may produce mostly incoherent and unnatural sentences, but achieves relative high accuracies when it comes to next word probabilities. Such a model may have low perplexity. We would like to argue here that one needs to look beyond perplexity when assessing the quality of language models. Specifically, we conjecture that longer range dependencies need to be more systematically captured in order to increase the fluency of sentences generated by language models.

\paragraph{Adverserial Learning} We would like to take inspiration from the recent success of generative adverserial networks (GANs) \cite{goodfellow2014generative} in image generation and focus less on prescribing the exact mechanism or features that are needed to improve language models, but rather focus on designing effective \textit{critics} that can discriminate between a natural sentence and one generated by a model. 

\bibliographystyle{acm}
\bibliography{th}

\end{document}
