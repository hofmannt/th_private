\documentclass{article}
\usepackage{amsmath,amsfonts,bm}
\usepackage{stmaryrd}

\newcommand{\w}{{\mathbf w}}
\renewcommand{\v}{{\mathbf v}}
\newcommand{\y}{{\mathbf y}}
\newcommand{\z}{{\mathbf z}}
\newcommand{\vZeta}{{\bm \zeta}}
\newcommand{\vRho}{{\bm \rho}}
\newcommand{\vOmega}{{\bm \omega}}
\newcommand{\vSigma}{{\bm \rho}}
\newcommand{\vNu}{{\bm \nu}}
\renewcommand{\b}{{\mathbf b}}
\newcommand{\I}{{\mathbf I}}
\newcommand{\E}{{\mathbf E}}
\newcommand{\mX}{{\mathbf X}}
\newcommand{\mZ}{{\mathbf Z}}
\newcommand{\mU}{{\mathbf U}}
\newcommand{\mA}{{\mathbf A}}
\renewcommand{\u}{{\mathbf u}}
\newcommand{\mV}{{\mathbf V}}
\newcommand{\x}{{\mathbf x}}
\renewcommand{\Re}{{\mathbb R}}
\newcommand{\mSigma}{{\mathbf \Sigma}}
\newcommand{\mLambda}{{\mathbf \Lambda}}
\newcommand{\mXi}{{\mathbf \Xi}}
\newcommand{\vXi}{{\bm \xi}}
\newcommand{\vAlpha}{{\bm \alpha}}
\newcommand{\vBeta}{{\bm \beta}}
\newcommand{\mGamma}{{\mathbf \Gamma}}
\newcommand{\mS}{{\mathbf S}}
\newcommand{\s}{{\mathbf s}}

\begin{document}

\paragraph{Ridge Regression} 
Given $n \ge d$ data points $(\x_i,y_i) \in \Re^d \times \Re$, generated i.i.d.~according to some unknown distribution $\mathcal P$, ridge regression aims at minimizing the regularized empirical objective 
\begin{align}
J(\w) :=  \frac 1{n} \left\| \mX^\top \w - \y \right\|^2 + \mu \left\| \w \right\|^2,\quad \mu \ge 0,
\end{align}
where $\mX = [\x_1  \dots  \x_n ] \in \Re^{d \times n}$ is the data matrix and $\y = (y_1,\dots, y_n)^\top \in \Re^n$ the vector of targets. We can rewrite the objective in terms of sufficient statistics of dimensionality independent of $n$. Expanding the squared norms yields
\begin{align}
J(\w) =  \w^\top \mS_\mu \w  -  2 \w^\top \s + \tfrac 1{n} \|\y\|^2, \;\;\; \text{where} \;\; 
\mS := \tfrac1n \mX \mX^\top, \;\; \s := \tfrac 1n  \mX \y\,,
\end{align}
and the shorthand $\mathbf A_\mu := \mathbf A + \mu \I$. 
%
The optimal solution is given explicitly by 
\begin{align}
\w_* := \arg\min_\w J(\w), \quad \w_* = \mS_\mu^{-1} \s\,.
\end{align}
As we can always include an intercept parameter, we assume that $\y^\top {\bm 1}=0$. In addition we fix the response scaling by requesting $\| \y\|^2=n$. Moreover, we also assume that the inputs have been centered, i.e.~$\mX {\bm 1} = {\bm 0}$. Note that $\s$ then represents the covariance vector between input dimensions and output.

\paragraph{Diagonalization} 

In order to simplify the analysis, we use the SVD $\mX / \sqrt n =   \mU \mSigma \mV^\top$, $\mSigma \in \Re^{d \times n}$. Then the covariance matrix diagonalizes
\begin{align}
\mS = \mU \mLambda \mU^\top, \quad \mLambda = \mSigma \mSigma^\top = \text{diag}(\lambda_1,\dots,\lambda_d)\,.
\end{align}
 Effectively, inputs are transformed into eigenfeatures via $\vXi :=  \mU^\top \x$ such that
%
\begin{align}
\tfrac 1n \mXi \mXi^\top = \mLambda, \quad \text{where} \quad \mXi := [\vXi_1\; \dots \; \vXi_n] = \mSigma \mV^\top\,.
\end{align}
Re-parameterizing $\v := \mU^\top \w$ and exploiting our assumptions, we can write
\begin{align}
J(\w) = \tilde J(\vOmega) = \vOmega^\top \mLambda_\mu \vOmega - 2 \vOmega^\top \mSigma  \vRho + 1, \;\;\text{where} \;\;  \vRho := \frac1n \mV^\top  \y \in \Re^n\,.
\end{align}
Note that we have defined $\vRho$ such that $\| \vRho\|=1$. The solution is given by 
\begin{align}
\vOmega_* = \mLambda_\mu^{-1} \mSigma \vRho = 
\begin{bmatrix}
\text{diag}\left( \frac{\sqrt{\lambda_i}}{\lambda_i + \mu} \right)
& \bm 0
\end{bmatrix}  \vRho\,,
\end{align}
and the residual at $\vOmega_*$ by 
\begin{align}
\tilde J(\vOmega_*) = 1 - (\mSigma \vRho)^\top \mLambda_\mu^{-1} (\mSigma \vRho)
= 1 - \| \vRho_{1:d}\|^2  =\| \vRho_{d+1:n} \|^2\,.
\end{align}
This residual is the length of the projection of $\y/n$ onto the kernel of $\mX$, which is spanned by the last $(n-d)$ columns of $\mV$.

\paragraph{Gradient Dynamics} 

In a time discrete setting, we fix a step size $\eta$ and study trajectories defined by the gradient descent operator 
\begin{align}
G_\eta(\w) := \w - \eta \nabla J(\w),  \;\; \w_{t+1} := G_\eta(\w_t), \;\; \w_0 \in \Re^d \,.
\end{align}
More explicitly we have
\begin{align}
&  \w_{t+1}  = \w_t - 2 \eta \left( \mS_\mu \w_t  - \s \right) =  \w_t - 2 \eta \mS_\mu \left(  \w_t  - \w_* \right)
\end{align}
so that by induction 
\begin{align}
\w_{t+1} - \w_* = \left(\I-2 \eta \mS_\mu \right)^t (\w_0 - \w_*)
\end{align}
A sufficient condition of  convergence for all initial $\w_0$ is that
\begin{align}
\| \I - 2\eta \mS_\mu\|_2 <1 \quad \Longrightarrow \quad 
\eta < \frac{1}{2} \cdot \frac{1}{ \lambda_{1} + \mu} \,.
\end{align}
%
Note that the slowest rate is in the direction of the smallest eigenvalue, namely it is strictly less than $1-\frac{\lambda_d+\mu}{\lambda_1 + \mu}$.
 
\paragraph{Suboptimality} 

We want to understand suboptimality of the iterate sequence generated by $G_\eta$ started at $\bm 0$
\begin{align}
\triangle J(\w_t) & := J(\w_t) - J(\w_*) = 
\end{align}


\paragraph{Dual Problem} The (negative) dual problem of  ridge regression is defined for variables $\vAlpha \in \Re^n$ 
\begin{align}
D(\vAlpha) = \frac{1}{\mu} \vAlpha^\top \mA_\mu \vAlpha - \frac 2n \vAlpha^\top \y, \quad 
\mA := \frac 1n \mX^\top \mX \,.
\end{align}
As before we can diagonalize 
\begin{align}
\mA_\mu = \mV \left( \tilde \mLambda + \mu \I \right) \mV^\top, \quad\text{where} \quad \tilde \mLambda = \mSigma^\top \mSigma = \begin{bmatrix}
\mLambda & \bm 0 \\ \bm 0 & \bm 0
\end{bmatrix}
\end{align}
As before, we can re-parameterize $\vBeta := \mV^\top \vAlpha$ and obtain a diagonalized problem with solution
\begin{align}
\vBeta^*  = \mu \I \left[ \mSigma^\top \mSigma + \mu \I \right]^{-1}  \mV^\top \frac \y n, \quad \vAlpha^* = \mV \vBeta^*
\label{eq:opt-dual-diag}
\end{align}
which needs to be compared to the primal solution, 
\begin{align}
\v^* = \mSigma  \left[ \mSigma^\top \mSigma + \mu \I \right]^{-1} \mV^\top \frac \y n, \quad \w^* = \mU \v^*,
\end{align}
Note that in the primal, we multiply with $\mSigma$ in the end, which projects away the $(n-d)$ directions in the kernel of $\mX$. These dimensions simply never play a role in the algorithm. In the (diagonalized) dual, however, these $(n-d)$ parameters need to be fitted, too. We can directly read off from Eq.~\eqref{eq:opt-dual-diag} that we also need to know the projection of $\y$ onto the kernel of $\mX$
\begin{align}
\beta^{I}_* = \frac 1n \mV_{I}^\top \, y , \quad I := [d+1:n]
\end{align}
Convergence in these dimensions is goverened by the rate $(1-2\eta)$, where $\eta < \frac 12 \frac{\mu}{\lambda_1+\mu}$. Note that thare are $(n-d)$ identical dimensions that all contribute to the solution suboptimality along a gradient trajectory of the dual problem. With zero initialization we have that $\beta^I_t = \left[ 1-(1-2\eta)^t \right] \mV_l^\top \y $




\newpage

Let us partition 
\begin{align}
\Re^{n \times n} \ni \mV = 
\begin{bmatrix}
\mV_{1} &  \mV_0
\end{bmatrix}, 
\quad \mV_{1} \in \Re^{n \times d}, \quad \mV_0 \in \Re^{n \times (n-d)}
\end{align}
Note that the first $d$ columns of $\mV$ span the image of the data matrix $\mX$, whereas the remaining $(n-d)$ columns span the kernel.
\begin{align}
n = \| \y \|^2 = \| \mV^\top \y \|^2 = \| \mV_1^\top \y\|^2 + \underbrace{\| \mV_0^\top \y \|^2}_{=: \delta n} 
\end{align}
The projection of $\y$ onto the kernel of $\mX$ does not show up in the primal, but it does show up in the dual. 




\newpage

Note that as we have normalized $\| \y\|^2=1$, $\mV^\top \y \le \bm 1$, so directions with small eigenvalues are automatically downweighted (directions with zero eigenvalue can be truncated), whereas a similar effect is not happening in the dual. 



Let us look at the dual in the degenerate limit of $\mLambda \to \bm 0$ in which case $\vAlpha^* \to \mV \mV^\top \y = \y$. Dual gradient descent will yield the sequence 
\begin{align}
\vAlpha_t  = \left[ 1-( 1-\eta \cdot \text{diag}(1+\lambda_i/\mu))^t \right]  \vAlpha^*
\stackrel{\mLambda \to \bm 0}{\longrightarrow}  \left[ 1-( 1-\eta)^t \right]  \y
\end{align}

How does dual sub-optimality evolve 
\begin{align}
\triangle D(\alpha_t) = \frac{1}{2\mu}
\end{align}


\newpage
Let us decompose $\vBeta = (\vBeta_1,\vBeta_0)$, where $\vBeta_1 \in \Re^d$ and $\vBeta_0 \in \Re^{n-d}$. Note that we can upper bound $\vBeta^*_0 \le \bm 1$ to account for dual parameters $\beta_i$ associated with eigenvalues $\lambda_i=0$. This means with a starting point $\vBeta_0 = 0$, we  


\end{document}