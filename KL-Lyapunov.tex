\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}
\title{Kurdyka-Lojasiewicz Property}
\author{Thomas Hofmann\\ ETH Zurich}

\newcommand{\E}{{\mathbf E}}
\renewcommand{\H}{{\mathcal H}}
\newcommand{\w}{{\mathbf w}}
\newcommand{\x}{{\mathbf x}}

\begin{document}
\maketitle

\noindent  Summary of \cite{attouch2013convergence}.\\

\noindent \textit{
I work under the assumption that risk functions represented by neural networks are real analytic (or can without much loss restricted appropriately). At least it is clear that this requirement is far more general than convexity.} 

Semialgebraic sets play a significant role as they are the basis to define semialgebraic functions. A key theorem in this context is the Tarski-Seidenberg Theorem\footnote{\url{en.wikipedia.org/wiki/Tarski_Seidenberg_theorem}} that shows that projections of semialgebraic sets to lower dimensions are semialgebraic (Theorem 2.2).

\paragraph{Kurdyka-Lojasiewicz property. }
The Kurdyka-Lojasiewicz property is stated in very precise terms here. First note that it is a pointwise property for $x^* \in \text{dom} \partial f$. An $\eta>0$ defines a level set slice 
\begin{align}
U_\eta(x^*) = U(x^*) \cap \bigcup_{s\in (0;\eta)} L_f(f(x^*)+s)
\end{align}
note that this definition is asymmetric in that it looks at levels sets strictly higher than $L_f(f(x^*))$.  The key object constructed is the de-singularizing concave function $\phi$
\begin{align}
\phi(0)=0, \quad \phi'(s) >0 \; (\forall s \in (0;\eta)), \quad (\Rightarrow \text{strict monotonicity, 1:1}) 
\end{align}
which then enters into the key inequality
\begin{align}
\phi'(f(x)-f(x^*)) dist(0,\partial f(x))  \ge 1
\end{align}
For $f$ being smooth,, finite-valued and shifted such that $f(x^*)=0$ one can rewrite this condition in a more interpretable manner as
\begin{align}
\| \nabla(\phi \circ f)(x) \| \ge 1
\end{align}
which shows that $\phi$ monotonically remaps $f$ such that gradients are bounded away from $0$. 

\paragraph{Iterative Methods}

Two key properties for discretized, iterative methods that are crucial are (H1, H2; there is a third one, more technical)
\begin{align}
& f(x^{k+1}) + a \|  x^{k+1} - x^k \|^2 \le f(x^k) \quad \text{Sufficient Decrease Condition ($a>0$)}\\
& \| w^{k+1}\| \le b \|  x^{k+1} - x^k \| \quad \text{Relative Error Condition ($b>0$)}
\end{align}
The (very technical) Lemma 2.6 basically states that for $f$ that are KL at $x^*$, such sequences will converge to a critical point, if initalized sufficiently close to $x^*$ (and provided some other technical stability conditions). The main theorems then are Theorem 2.9 \& 2.10, where convergence and finitness of the partially linear  paths of the iterates is shown (for critical points and local minima, respectively).

\paragraph{Inexact Proximal Algorithm}

One of the applications of particular interest is Algorithm 2. Starting point is the formulation of an exact proximal point method in Eq.~(33-35). The decrease condition is lowered by a factor $\theta$ (Eq.~(36)) and the optimality condition is weakend in Eq.~(38) and further in Eq.~(39). The question is how to really turn this into an algorithm (as it is not an algorithm -- ``find bla bla").

\newpage


\noindent 
Kurdyka-Lojasiewicz property: for some $\theta \in [\tfrac 12; 1)$, $C>0$
\begin{align}
| f(x) - f(x^*) |^\theta \le C \| \nabla f(x) \|, \; \forall x \in U(x^*)
\end{align}
% see proof in: https://www.ljll.math.upmc.fr/~plc/attouch.pdf
With the help of this inequality Lojasiewicz's theorem \cite{Lojasiewicz84} shows convergence (and finite trajectory length) of gradient dynamics.  This work has been extended in \cite{kurdyka2000proof} 
% http://emis.ams.org/journals/Annals/152_3/mostowski.pdf

The basic proof technique for Lojasiewicz's theorem defines a Lyaponov function via $\phi(f(x(t))-f(x^*))$, where $\phi(s) = Cs^{1-\theta}$ is a desingularizing function. This construction can also be used \cite{kurdyka1998gradients} to extend these results via the Kurdyka-Lojasiewicz (KL) inequality \cite{attouch2010proximal} to the non-smooth case. There are many interesting function classes that fufill the KL inequality, including semi-algebraic functions\footnote{See presentation: \url{www.ljll.math.upmc.fr/~plc/attouch.pdf}}. Convergence results for steepest descent of analytic functions can be found in \cite{absil2005convergence} and extensions to proximal methods for non-smooth optimization are presented in \cite{attouch2009convergence,attouch2011prox}. More results for splitting (forward-backward) algorithms as well as use cases in compressed sensing (and more) are summarized with references in \cite{attouch2011Poincare}.

\section{Monotone Operators}

The theory of monotone operators is extensive. A recent comprehensive overview can be found in \cite{ryu2016primer}. The key idea is to define problems such that solutions correspond to fixed points of an operator, i.e.~$x = A(x)$. Think of the case of finding a critical point in optimization, where we may consider gradient operators $A=\nabla f$ (or $A=\partial f$). Similarly, in finding saddle points or solving zero-sum games, we may have operators $A=(\nabla_\phi f, - \nabla_\psi f)$ or more generally $A=(\nabla_\phi f, \nabla_\psi g)$. Solutions to such problems can often be obtained by studying the ODE $\dot x + A(x) = 0$ and discretized versions thereof such as (in the simplest case) $x_{n+1} = x_{n} - \eta A(x_n)$. 

\subsection{Maximal Monotonicity} 

The monotonocity of $A$ is a fundamental property that generalizes many of the properties of gradient operators. Formally, a relation $R$ over a vector space is monotone, if 
\begin{align}
(u-v)^\top (x-y) \ge 0, \quad \forall (x,u), (y,v) \in R
\end{align}
and an operator is monotone, if the induced relation $R=(x,A(x))$ is. A technically important property is maximal montonicity, which means, there is no extension $R' \supset R$ such that $R'$ is still monotone. Note that gradients (or sub-differentials) of convex functions are maximal monotone. Similar of how convexity can be generalized to strong convexity, one can define strong monotoncity by requiring $(A(x)-A(y))^\top (x-y) \ge \mu \| x-y\|^2_2$ for some $\mu >0$. For convex functions $f$, one usually requires $f$ to be also closed and proper (so-called CCP) as then $\partial f$ is guarenteed to be maximally monotone \cite[Section 4.2]{ryu2016primer}. Moreover note that projections are always monotone \cite[Section 4.3]{ryu2016primer}. For the maximal monotonicty of the saddle subdifferential for convex-concave problems see \cite{rockafellar1976monotone}. It is also possible to cast Lagrangian optimization in this framework by defining the KKT operator \cite[Section 4.4]{ryu2016primer}.

\subsection{Fixed-Point Iterations}

It is often possible to find a fixed point of an operator $F$ by iterating $x_{n+1} = F(x_n)$, which is also known as Picard's algorithm \cite{picard1890memoire,banach1922operations}. One condition under which convergence is guaranteed is operator \textit{contractiveness} cf.~\cite[Section 5.1]{ryu2016primer}. Assume that we have a $\mu$-monotone and $L$-smooth operator $A$, then we can perform updates of the type $x_{n+1} = x_n - \eta A(x)$ such that $F = I - \eta A$. It can be shown that $F$ is contractive for $\eta \in (0; 2\mu/L^2)$, which is worse than what one gets for the specific case of gradient descent. Contractive fixed point iterations are not just converging, but they converge linearly and are Fej\'er monotone\footnote{$\| x_n -x\|$ is strictly decreasing with $n$.}. 

Sometimes it is interesting to consider averaged operators that can be written $F = (1-\theta)I + \theta G$ for $\theta \in (0;1)$ and $G$ non expansive. Such damping can sometimes guarantee convergence for non-expansive operators, e.g.~with typical $1/n$ rates, see \cite[Section 5.2]{ryu2016primer}.

On a more abstract level, one can identify $R=(I+\eta A)^{-1}$ as the resolvent of $A$ and $C = 2R -I$ as the Cayley operator (reflected resolvent). These operators are related in interesting ways \cite[Section 6.1]{ryu2016primer}. $R$ is an averaged operator while $C$ is not, therefore it is often better to base a fixed point iteration on $R$. This is nothing esle than the \textit{proximal point method}, $x_{n+1} = (I + \eta A)^{-1}(x_n)$, cf.~\cite{martinet1970breve,rockafellar1976monotone}. In this context, one can also analyse the methods of multipliers.\footnote{TH: Follow up on this...}

Another way to define fixed-point schemes is to use \textit{operator splitting} \cite[Section 7]{ryu2016primer}, e.g.~if $F = A+B$, where $A$ and $B$ are maximally monotone and have some other properties. For instance if $A$ is singly-valued than one can perform forward-backward splitting 
\begin{align}
x_{n+1} = R_B(x_{n} - \eta A(x_n))
\end{align}
For instance, $A$ could be a gradient operator and $B$ be a more general operator related to a non-differentiable objective. The paradigmatic case is the proximal gradient method \cite{combettes2005signal,duchi2009efficient,beck2009fast} were we optimize a convex $f=g+h$ with differentiable $g$ via:
\begin{align}
x_{n+1} = 	\arg\min_x \left[ 
g(x_n) + \nabla g(x_n)^\top (x-x_n)+ h(x) + \frac{1}{2\eta} \| x - x_n\|^2
\right] 
\end{align}
This uses a first order approximation of $g$ at the current iterate. It converges for differentiable $g$ with $L$-Lipschitz gradients and step size $\eta <2/L$. More complex splitting methods are discussed in \cite[Section 7.2-7.4]{ryu2016primer}.

\subsection{Specific Algorithms}

There are numerous examples discussed in  \cite[Section 7.5]{ryu2016primer}. We focus on saddle point and game-related problems. The solution presented here relates the forward-backward-forward splitting (as a special case) to the extragradient method \cite[Section 7.5, p.~41]{ryu2016primer}. The convergence proof for the convex-concave case follows from the general result(s). 

\section{Proximal Methods}

In optimization, the more specific setting of the proxcimal operator has been studied and used extensively. The prox-operator is defined as
\begin{align}
\text{prox}_{\lambda f}(y) = \arg\min_x \left\{ f(x) + \frac 1{2\lambda} \| x - y \|^2_2 \right\}
\end{align}

\section{Variational Inequalties}

There is an immense body of literature and knowledge on (finite dimensional) variational inequalties, which generalize many settings from optimization to game theory and econometrics. One of the authoritative monographs on the subject is \cite{facchinei2007finite}. Seminal work includes the fixed-point algorithm of Scarf \cite{scarf1967approximation}, which led to the development of homotopy methods for the field of equilibrium programming. One of the more modern developments are variants of Newton's method, in particular \cite{de1996semismooth}. 

\bibliographystyle{acm}
\bibliography{IterativeNumerics}

\end{document}

\newpage

\paragraph{Proximal Method for Monotone Operators}

Given a strongly monotone operator (generalizing a gradient operator), the proximal method is a method for finding points $x^*$ that fulfill $A(x^*)=0$, which proceeds in iterations of the type 
\begin{align}
x_{t+1} = x_t + \eta_t A(x_{t+1}) 	
\end{align}
Note that this equation is implicit in that $x_{t+1}$ (and not $x_t$) appears on the right-hand side. It generalizes the implicit Euler method for integrating a simple ODE $\dot x = -\nabla f(x)$. Note that a standard method for solving the (inner) proximal problem are fixed-point iterations\footnote{\url{en.wikipedia.org/wiki/Backward_Euler_method}}. A special case of performing fixed-point iterations only once results in the extra-gradient approach\footnote{If I understand this correctly, needs to be verified.}
\begin{align}
\bar x_{t+1} = x_t + \eta_t A(x_{t}), \quad x_{t+1} = x_t + \eta_t A(\bar x_{t+1}). 
\end{align}
The general proximal approach has been suggested and investigated in the classical paper by Rockafellar \cite{rockafellar1976monotone}. 
% http://www.cs.cmu.edu/~suvrit/teach/papers/1976_rockafellar_monotone_operators_proximal_point_algorithm.pdf
There it is introduced by observing that (under suitable conditions on the operator) there always is a unqiue $x^+$ satisfying 
\begin{align}
x^+ = x + \eta A(x^+), \quad \text{s.t.} \quad x^+ = \underbrace{(I - \eta A)^{-1}}_{:=P}x
\end{align}
where $P$ is called the proximal mapping, which is shown to be  non-expansive. A stationary point $x^*$ of $P$ fulfills $A(x^*)=0$. One can often recast the problem in terms of a local optimization problem, e.g.~for gradient operators 
\begin{align}
x_{t+1} = \arg\min_x \left\{ f(x) + \frac1{2\eta} \| x - x_t\|^2 \right\}
\end{align}
which is the caonical definition in the optimization literature (e.g.~Boyd et al), where the mofied objective is sometimes also called Moreau-Yosida regularization. Similar formulations are possible for saddle points of convex-concave functions (\cite{rockafellar1976monotone}, Eq.~(1.11)). The mathematically most relevant questions center around weak (or strong) convergence of such iterative methods, given general properties of the monotone operator. There are also results that investigate the underlying ODE for the subgradient dynamics of convex functions, e.g.~\cite{bruck1975asymptotic}. 



\newpage

\noindent 
\subsection*{Numerics of GANs \cite{mescheder2017numerics}}
\paragraph{Basic Setting} Bi-objective optimization. Parameters $\phi$, $\theta$, $\xi = (\phi,\theta)$. Two coupled objectives, $f$ and $g$, special case: zero-sum games $g=-f$. Goal find \textit{saddle point}, i.e.~pair $(\phi^*, \theta^*)$ such that 
\begin{align}
\phi^* \in \arg\min_{\phi \in \Phi} f(\phi,\theta^*), \quad 
\theta^* \in \arg\min_{\theta \in \Theta} g(\phi^*,\theta)
\end{align}
where $\Phi, \Theta$ denote local or global domains. 
Define a vector field and its Jacobian field as
\begin{align}
v= \begin{pmatrix}  \nabla_\phi f \\ \nabla_\theta g \end{pmatrix}, \quad 
v' =\begin{pmatrix} \nabla^2_\phi f & \nabla_\phi \nabla_\theta g \\ \nabla_\phi \nabla_\theta f  & \nabla_\theta^2 g 
\end{pmatrix}, \quad  
\end{align}
In the special case of $g=-f$, one has that $v'$ positive (semi) definite.~iff both Hessians are positive (semi) definite\footnote{Follows directly from block matrix multiplication.}.

\paragraph{Fixed-points of Operators} One is interested in iterative methods to find saddle points, which often amount to the repeated application of an operator $F$. Such a method converges locally around $x_0$ with a linear rate, if $F'$ (the Jacobian of $F$) has a bounded spectral radius: fixed point theorem
\begin{align}
F(x_0) = x_0, \quad \rho(F'(x_0)) < 1 \; \Longrightarrow \exists \epsilon \; \forall x \in U_\epsilon(x_0), \; F^t(x) \to x_0
\end{align}
A typical iterative method (e.g.~gradient descent) makes an additive correction to the current iterate and thus we may be intersted in the specific case where $F = \text{id} - \eta G$, such that $F' = I - \eta G'$. 

Let us first look at the special case where $G =\nabla f$ is a gradient operator (single minimization problem). In the latter case convergence is tied to local convexity, i.e.~a p.d.~condition on the Hessian $G'$, namely $G' \succ 0$.  The learning rate is upper-bounded by $\lambda_{\max}(G')$ as $|1 - \eta \lambda_{\max}|<1$ and hence $\eta < 2/\lambda_{\max}$. As $G'$ is symmetric, all eigenvalues $\lambda$ are real.

In the more general case discussed in \cite{mescheder2017numerics}, $G$ is a more general vector field and $G'$ is no longer symmetric, allowing for the possibility of complex eigenvalues  with non vanishing imaginary parts. As their main technical Lemma 4 shows, the above condition can be generalized to yield (for p.d.~$G'$)
\begin{align}
\eta < \frac{2}{\text{Re}(\lambda)} \left( 
\frac{\text{Re}(\lambda)}{|\lambda|}\right)^2 
\end{align} 
Hence, in addition to the real part, one needs to also take the imaginary part into account (the modulus, relative to the real part of an eigenvalue).  This is a nice and basic piece of analysis.

\paragraph{Improving Local Stability} The paper then suggets to modify the objectives $f$ and $g=-f$ by adding a ``consensus term'', i.e.~a shared objective, which is simply chosen to be $\Omega = \frac \gamma 2 \| v \|^2$. The idea is that a large enough choice of $\gamma$ will lead to better convergence properties and larger learning rates (i.e.~faster training). Note also that at a saddle point $\xi^*$ (Nash equlibrium) $v(\xi^*)=0$ and hence this modification is not changing the location (and number) of saddle points.  However, the modification may make instable criticial points locally stable, which is clearly non-desirable and points at a possible conceptual flaw.

\paragraph{Accelerated Gradients?}

The modified vector fields are (as one can easily derive) given by $w = v + \gamma v' v$. For $\gamma=0$ we can think of simultaneous gradient descent as a time discretized version (direct Euler method) for solving an underlying ODE, namely $\dot\xi = - v$, describing a massless particle moving through the vector field. 

The additional term can be thought of as representing the acceleration in the above system. As a system evolving as $\dot\xi = -v$ has an acceleration field $\ddot\xi = - \dot v = v' \dot \xi= - v'v$. In the time-continuous domain we may  thus be compelled to write in terms of ODE
\begin{align}
\dot \xi + w = 0 \iff \dot \xi + v + \gamma v' v = 0 \stackrel{?}{\iff} -\gamma \ddot\xi + \dot\xi + v =0
\end{align}
However this would not be correct as the identity $\ddot\xi = - \dot v$ only holds for a solution to the orginal system $\do \dot\xi = - v$. If we modify the ODE however, the solution of this ODE will (in general) no longer fullfill this as now the dynamics has changed.

\subsection*{Solving GANs via Proximal Methods}

\paragraph{Proximal Method for Monotone Operators}

Given a strongly monotone operator (generalizing a gradient operator), the proximal method is a method for finding points $x^*$ that fulfill $A(x^*)=0$, which proceeds in iterations of the type 
\begin{align}
x_{t+1} = x_t + \eta_t A(x_{t+1}) 	
\end{align}
Note that this equation is implicit in that $x_{t+1}$ (and not $x_t$) appears on the right-hand side. It generalizes the implicit Euler method for integrating a simple ODE $\dot x = -\nabla f(x)$. Note that a standard method for solving the (inner) proximal problem are fixed-point iterations\footnote{\url{en.wikipedia.org/wiki/Backward_Euler_method}}. A special case of performing fixed-point iterations only once results in the extra-gradient approach\footnote{If I understand this correctly, needs to be verified.}
\begin{align}
\bar x_{t+1} = x_t + \eta_t A(x_{t}), \quad x_{t+1} = x_t + \eta_t A(\bar x_{t+1}). 
\end{align}
The general proximal approach has been suggested and investigated in the classical paper by Rockafellar \cite{rockafellar1976monotone}. 
% http://www.cs.cmu.edu/~suvrit/teach/papers/1976_rockafellar_monotone_operators_proximal_point_algorithm.pdf
There it is introduced by observing that (under suitable conditions on the operator) there always is a unqiue $x^+$ satisfying 
\begin{align}
x^+ = x + \eta A(x^+), \quad \text{s.t.} \quad x^+ = \underbrace{(I - \eta A)^{-1}}_{:=P}x
\end{align}
where $P$ is called the proximal mapping, which is shown to be  non-expansive. A stationary point $x^*$ of $P$ fulfills $A(x^*)=0$. One can often recast the problem in terms of a local optimization problem, e.g.~for gradient operators 
\begin{align}
x_{t+1} = \arg\min_x \left\{ f(x) + \frac1{2\eta} \| x - x_t\|^2 \right\}
\end{align}
which is the caonical definition in the optimization literature (e.g.~Boyd et al), where the mofied objective is sometimes also called Moreau-Yosida regularization. Similar formulations are possible for saddle points of convex-concave functions (\cite{rockafellar1976monotone}, Eq.~(1.11)). The mathematically most relevant questions center around weak (or strong) convergence of such iterative methods, given general properties of the monotone operator. There are also results that investigate the underlying ODE for the subgradient dynamics of convex functions, e.g.~\cite{bruck1975asymptotic}. 

\paragraph{Proximal Iterations with Momentum}
In the spirit of the approach first proposed in \cite{antipin1994minimization} to use ODEs as the starting point for optimization algorithms, \cite{alvarez2001inertial} investigates the heavy ball with friction ODE 
\begin{align}
\ddot x + \gamma \dot x + \nabla f(x) = 0 
\end{align}
which can be shown (citation?) to converge towards the global minimum of a (strongly) convex function. There is also work in the mathematics literature (and not just in optimization as Nesterov's \cite{nesterov1983method}), e.g.~\cite{attouch2000heavy,alvarez2000minimizing}. Understanding the dynamics of such system seems to be easier for coercive operators (what is the relevance of that?) \cite{attouch2000heavy}.
% http://www.dim.uchile.cl/~falvarez/Papers/AlvAtt00.pdf 

% IMORTANT 
% https://arxiv.org/pdf/1609.08177.pdf

% https://www.ljll.math.upmc.fr/~plc/attouch.pdf

% https://link.springer.com/article/10.1007/s10957-005-7564-z


\newpage

They take this as the starting point to motivate a time discrete version of an operator problem. Assuming $A$ is a maximal monotone operator (a generalization of a gradient operator), then they define an implicit iterative approach via 
\begin{align}
x_{t+1} - x_t - \alpha_t  (x_t - x_{t-1}) + \lambda_t A(x_{t+1}) = 0 
\end{align}





\paragraph{Variational Inequalities} A standard approach in mathematics deals with solving variational inequalities (VIs). Here one has an operator $G: U \to \Re^n$ (see \cite{konnov2002theory} for an overview) and is interested in the existence and computation of points $x^*$ such that
\begin{align}
\langle G(x^*), x-x^* \rangle \geq 0, \quad \forall x \in U
\end{align}
for which one also considers the dual VI problem
\begin{align}
\langle G(x), x-x^* \rangle \geq 0, \quad \forall x \in U
\end{align}
The paradigmatic case that is generalized via VIs is optimization where we have a minimization problem and $G$ is the gradient operator of $f$, the function to be minimized (assume $U = \Re^n$ for simplicity). Assume $f$ is convex, then DVI characterizes the global minimum. 


\paragraph{Extragradient Methods} In the optimization literature, the above problem is known as the problem of solving \textit{variational inequalities} and a standard tool to solve these problems is the extragradient method \cite{Korpelevich76Extragradient,taskar2006structured}. Intersting quote from \cite{nguyen2016extragradient}: ``Intuitively, this additionalt
iteration allows to foresee the geometry of the problem and take into account curvature information,
one of the most important bottlenecks for first order methods.''



% Interesting, use of Fenchel duality to construct gap function for equilibrium problems
% http://www.mat.univie.ac.at/~rabot/publications/jour06-07.pdf

% Somewht advanced paper by the expert of VIs, Noor (Pakistan)
% shttps://www.researchgate.net/profile/Muhammad_Noor3/publication/275			508330_General_equilibrium_bifunction_variational_inequalities/links/553de2750cf2c415bb0f7e9e/General-equilibrium-bifunction-variational-inequalities.pdf

% check this: http://www.dim.uchile.cl/~falvarez/Papers/AlvAtt01b.pdf

% What are these splitting operators - SIAM journal s		
% https://scholar.google.com/scholar?cluster=7851196261519148778&hl=en&as_sdt=0,5&as_vis=1

% http://www.pmf.ni.ac.rs/pmf/publikacije/filomat/2015/29%20-%205/F29-5-8%20-%20597.pdf

\end{document}

\newpage


Convergence guarantee via Jacobian 
\begin{align}
F' = I + h A, \quad \rho(F') <1 \iff h < \frac{1}{|\text{Re}(\lambda)|} \cdot \frac{2}{1+ \left( \frac{\text{Im}(\lambda)}{\text{Re}(\lambda)} \right)^2} \; (\forall \lambda)
\end{align}

\noindent 
Add regularizer $\Omega(\xi) = \frac 12 \| v(\xi)\|^2$ and  regularized function and vector fields 
\begin{align}
\bar f  = f  - \gamma \Omega, \; \bar g = g - \gamma \Omega, \quad 
w = v - \gamma \nabla \Omega, \quad \nabla \Omega = \left( v'^\top \right) v
\end{align}
So the fixed point function of gradient updates is given by 
\begin{align}
F(\xi) = \xi + h\left[I - \gamma v'(\xi) \right] v(\xi), \quad \text{more generally} \quad [...] =:A
\end{align}


\end{document}
