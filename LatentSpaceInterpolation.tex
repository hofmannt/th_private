\documentclass{article}
\usepackage{amsfonts,amsmath}
\newcommand{\x}{{\mathbf x}}
\newcommand{\z}{{\mathbf z}}
\renewcommand{\v}{{\mathbf v}}
\newcommand{\U}{{\mathbf U}}
\newcommand{\expect}[1]{{\mathbb E}\left[{#1}\right]}
\author{Thomas Hofmann}
\title{Unbiased Latent Space Interpolation}
\begin{document}

\maketitle

In sampling from implicit models, one often interpolates between two randomly drawn code words $\Re^d \ni Z_0, Z_1 \sim P_Z$ along straight lines. However, the induced distribution for points on such an interpolant may be very different from the original distribution. Let us define the mid-point distribution $Y = \frac 12 ( Z_0 + Z_1) \sim P_Y$. We are particularly interested in cases, where $P_Z$ is isotropic, in which case $P_Y$ will also be isotropic. More specifically, we want to focus on distributions in which coefficients

 Of interest is then the induced one-dimensional distribution of the (squared) radii,
\begin{align}
R := \| Z\|^2 \sim P_R, \quad  \text{and}  \quad S :=  \left\|  \frac{Z_0 + Z_1}{2} \right\|^2 \sim P_S \,.
\end{align}


Let us use moment generating functions to shed some light on the situation. Assume that $Z$ has a moment generating function $M_Z(\phi) = \expect{\exp[\langle \phi, Z \rangle]}$. Note that $R=\sum_i Z_i^2$.

\newpage

 and $M_R(t) = \prod_i M^2_{Z_i}(t)$. Note that in the isotropic case all $M_{Z_i}$ are identical. 

\newpage




In the isotropic case, each coefficient of $Z_i$ follows the same distribution with $M_{z}$ and $R = \sum_i Z_i^2$.


\newpage


Note that if the moment generating function of $R$ exists, then 
\begin{align}
M_R(t) = \expect{e^{tR}}, \quad 
M_S(t) = \left[ M_R(\tfrac t4)  \right]^2 
= \expect{e^{\tfrac {tR}4 }}^2 
\end{align}

For concreteness assume that $P_Z$ is an isotropic normal with unit variance. Then $R \sim \chi^2(d)= \Gamma(k,2)$ where $k = \tfrac d2$ and consequently
\begin{align}
M_R(t) = (1-2t)^{-k} , \quad M_S(t) = \left(1- t \right)^{-2k}
\end{align} 
and it easy to see that $S \sim \Gamma(2k,1)$. The first moments are then given by 
\begin{align}
\expect{R} = \frac{d}{dt} M_R(0)  = 2k, \quad \expect{S} = \frac{k \theta }{2}
% \frac d{dt} M_S(0)
\end{align}
???

\newpage



\paragraph{Interpolation}  Let us assume we define an isotropic distribution over random vectors $\x \in \Re^d$. We are interested in how to interpolate between points $\x_0, \x_1$ such that the distribution of mid-points does not diverge much from the original distribution. We will by $f: [0;1] \times (\Re^d \times \Re^d) \to \Re^d$ an interpolation scheme such that for any two points $\x_0$, $\x_1$, $f(\cdot; \x_0,\x_1)$ is a smooth curve with $f(0)= \x_0$ and $f(1) = \x_1$. Linear interpolation is defined as $f(t; \x_0,\x_1)= (1-t) \x_0 + t \x_1$. We will often denote the midpoint by $\z = f(\tfrac 12; \x_0,\x_1)$. Given a distribution on $\x$ it induced a unique distribution on $\z$. Let us assume the moment generating function of $\x$ exist, then for linear interpolation 
\begin{align}
M_x(\phi) = \expect{e^{\phi^\top \x}}, \quad M_z(\phi) = \left[ M_x(\tfrac 12\phi)  \right]^2 \,.
\end{align}

\paragraph{One-Dimensional Case} Let us look at the linear case and assume $x \sim \Gamma(k,\theta)$, then for $t < 1/\theta$:
\begin{align}
M_x(t) = (1-t\theta)^{-k} , \quad M_z(t) = \left(1-t \frac \theta 2 \right)^{-2k}
\end{align} 
Hence $z \sim \Gamma(2k,\tfrac \theta 2)$. What choice of $k$ and $\theta$ yields the smallest divergence? We use the general result for KL-divergences in the exponential family with natural parameters $\eta$
\begin{align}
\text{D}(q||p) = g(\eta_p) - g(\eta_q) - (\eta_p - \eta_q) \nabla g(\eta_q)
\end{align}
where $g$ is the log partition function. For the $\Gamma$-distribution we have
\begin{align}
\eta = (k-1, - \tfrac 1 \theta), \quad g(\eta) = \ln\Gamma(k)+\ln \theta
\end{align}
and thus 
\begin{align}
\frac{\partial g}{\partial [k-1]} = \frac{\Gamma'(k)}{\Gamma(k)}, \quad 
\frac{\partial g}{\partial [-\tfrac 1\theta]} = \frac{\partial \ln(-1/\theta)}{\partial \theta} = - \theta \cdot \frac 1{\theta^2} = - \frac 1 \theta
\end{align}
So if $p= \Gamma(k,\theta)$ and $q= \Gamma(k',\theta')$ then 
\begin{align}
\text{D}(q||p) = \ln \frac{\Gamma(k)}{\Gamma(k')} + \ln \frac{\theta}{\theta'} + (k'-k) \psi(k') \
+ \frac{\theta - \theta'}{\theta'}
\end{align}

\newpage


First, we would like to make sure that the distribution induced on $\z$ is also isotropic. In general, this will turn out to be not too difficult. For instance, wer may require rotation invariance of the interpolation scheme 
\begin{align}
f(\cdot; \U \x_0,\U \x_1) = \mathbf U^\top f(\cdot; \x_0,\x_1), \quad \forall \; \text{orthogonal} \; \U 
\end{align}
which is a very natural condition. Second, and this poses more difficulties, we also want to make sure that the distribution of the norm of midpoint vectors does not diverge much. The problem is that if we generate $\x$ by $d$ i.i.d.~random numbers (e.g.~for each dimension), then $\| \x\|$ concetrates around its expectation. For instance, in the Gaussian case $\|\x\|^2 \sim \chi^2(d)$. Now the expectation of the norm of midpoints may yield a significantly smaller value, for instance, when using linear interpolation, there will be a bias towards smaller norms.\\ 

\noindent Let us focus on the normal case. Note that $\|\x\|^2 \sim \chi^2(d) = \Gamma(\frac d2,2)$. The distribution of $\| \x_1 + \x_2\|^2$ is simply $\chi^2(2d)$, but we need to scale this by a factor of $\tfrac 14$, which yields the distribution $\Gamma(d,\frac 12)$. The divergence between these distributions grows (asymptocially) linear with $d$, which is problematic. \\

\noindent Let us see in general, how the distributions relate, when we perform linear interpolation. 
\begin{align}
p_{\|\z\|^2}(r^2) = \int \int \delta\left(r^2 - \frac14 \| \x_0 + \x_1\|^2 \right) \, p(\x_0) p(\x_1) \; d\x_0 \, d\x_1
\end{align}
To turn that into something more practical, let us exploit isotropy, so that we only need to care about the angle $\theta$, and the radii $r_0$, $r_1$. Note that 
\begin{align}
\frac 12 \| \x_0 + \x_1\|^2 = \frac 12 \| \x_0\|^2 + \frac 12 \| \x_1\|^2 + \langle \x_0, \x_1 \rangle = \frac 12 r_0^2 + \frac12 r_1^2 + r_0 r_1 \cos \theta
\end{align}
and thus
\begin{align}
p_{\|\z\|^2}(r^2) & = \frac 1 \pi \int_{0}^\pi \int_0^\infty \int_0^\infty \delta\left(r^2 - \frac 12 r_0^2 -\frac 12 r_1^2 -  r_0 r_1 \cos \theta \right)\, p_{\|\x\|^2}(r_0^2) p_{\|\x\|^2} (r_1^2) \; dr_0 \, dr_1 d\theta
\end{align}



\end{document}